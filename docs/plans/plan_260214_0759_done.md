# Plan: BenchmarkSample — BenchmarkDotNet Performance Suite ✅ DONE

**Created:** 2026-02-14 07:59  
**Status:** ✅ Completed (2026-02-14)  
**Roadmap item:** #11 BenchmarkSample — BenchmarkDotNet Performance Suite

---

## Goal

Create a reproducible BenchmarkDotNet performance suite in `samples/BenchmarkSample/` that measures embedding generation, tokenization, and cosine similarity throughput. Establish a cross-platform baseline strategy for Windows and Linux to detect performance regressions.

---

## Deliverables

### 1. BenchmarkSample project (`samples/BenchmarkSample/`)

| File | Purpose |
|------|---------|
| `BenchmarkSample.csproj` | Console project referencing BenchmarkDotNet + main library |
| `Program.cs` | Entry point with `BenchmarkSwitcher` |
| `EmbeddingBenchmarks.cs` | Single-text and batch embedding benchmarks |
| `SimilarityBenchmarks.cs` | Cosine similarity benchmarks (TensorPrimitives) |
| `TokenizerBenchmarks.cs` | Tokenizer throughput benchmarks |
| `README.md` | Usage instructions and baseline strategy documentation |

### 2. Baseline strategy

| Artifact | Purpose |
|----------|---------|
| `baselines/` directory | Stores exported BenchmarkDotNet JSON results per platform |
| `baselines/README.md` | Documents the baseline capture and comparison workflow |
| Naming convention | `{os}-{arch}-{date}.json` (e.g. `windows-x64-20260214.json`) |

### 3. Cross-platform execution strategy

- **Local:** Run `dotnet run -c Release` from the BenchmarkSample directory on each target OS.
- **CI (future):** Add a GitHub Actions workflow that runs benchmarks on `windows-latest` and `ubuntu-latest`, exports JSON artifacts, and (optionally) compares against stored baselines using `ResultComparer` from the `dotnet/performance` repo.
- **Regression detection:** Use BenchmarkDotNet's `--exporters json` to produce machine-readable results. A manual or CI-driven comparison step diffs new results against baselines and flags significant regressions (>5% mean increase).

### 4. Documentation & solution updates

- Add `BenchmarkSample` to `ElBruno.LocalEmbeddings.slnx`.
- Update `samples/README.md` with BenchmarkSample description.
- Update roadmap item #12 to ✅ Completed.

---

## Benchmark scenarios

| Benchmark class | Method | Parameters | Measures |
|-----------------|--------|------------|----------|
| `EmbeddingBenchmarks` | `SingleEmbedding` | — | Throughput for 1 text |
| `EmbeddingBenchmarks` | `BatchEmbedding` | count: 10, 50, 100 | Batch throughput |
| `SimilarityBenchmarks` | `CosineSimilarity` | dimensions: 384, 768 | TensorPrimitives cosine perf |
| `SimilarityBenchmarks` | `FindClosest` | corpus: 100, 1000 | Search / ranking perf |
| `TokenizerBenchmarks` | `TokenizeSingle` | — | Single-text tokenization |
| `TokenizerBenchmarks` | `TokenizeBatch` | count: 10, 50 | Batch tokenization |

---

## Implementation steps

1. Create `samples/BenchmarkSample/BenchmarkSample.csproj`.
2. Create `Program.cs` with `BenchmarkSwitcher.FromAssembly(typeof(Program).Assembly).Run(args)`.
3. Create `EmbeddingBenchmarks.cs` with `[MemoryDiagnoser]` and `[GlobalSetup]` for model loading.
4. Create `SimilarityBenchmarks.cs` with pre-computed vectors.
5. Create `TokenizerBenchmarks.cs` with the tokenizer initialized in `[GlobalSetup]`.
6. Create `README.md` with run instructions and baseline strategy.
7. Create `baselines/README.md` documenting the baseline workflow.
8. Add the project to the solution file.
9. Build and validate.
10. Update `samples/README.md`.
11. Mark roadmap item #12 as completed.

---

## Baseline strategy detail

### Capturing a baseline

```bash
# Windows
cd samples/BenchmarkSample
dotnet run -c Release -- --exporters json --artifacts baselines/windows-x64

# Linux
cd samples/BenchmarkSample
dotnet run -c Release -- --exporters json --artifacts baselines/linux-x64
```

### Comparing against a baseline

Use `ResultComparer` from the [dotnet/performance](https://github.com/dotnet/performance) tools:

```bash
dotnet run --project ResultComparer -- \
  --base baselines/windows-x64 \
  --diff BenchmarkDotNet.Artifacts \
  --threshold 5%
```

Or a simpler manual comparison:

1. Run benchmarks and export JSON.
2. Compare mean values against the stored baseline JSON.
3. Flag any benchmark where the new mean exceeds the baseline mean by more than 5%.

### CI integration (future)

A GitHub Actions workflow can:

1. Run benchmarks on `windows-latest` and `ubuntu-latest`.
2. Export results as JSON build artifacts.
3. Download the stored baseline from the `baselines/` directory.
4. Run a comparison step and post results as a PR comment.

This is not implemented in this plan to avoid modifying `.github/` configuration without explicit need.

---

## Risk / Notes

- BenchmarkDotNet requires Release configuration for accurate results.
- Model files must be pre-downloaded on the benchmark machine.
- `[GlobalSetup]` loads the model once; the model load time itself is not benchmarked in the tight loop (it's a one-time cost).
- The `baselines/` directory is committed to the repo but `.gitignore`d for the `BenchmarkDotNet.Artifacts/` output directory.
