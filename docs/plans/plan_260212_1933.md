# Plan: RAG Samples + README Restructure

> **Created:** 2026-02-12 19:33

## Summary

Two new standalone RAG sample projects using `elbruno.LocalEmbeddings` for embeddings and **phi-3.5-mini** as the chat LLM — one via Ollama, one via Foundry Local. The main README gets trimmed to a "hello world" quick start; detailed content moves to `docs/`. The existing ConsoleApp and RagChat samples are kept as-is.

## Steps

### 1. Create `samples/RagOllama/` project

- New `samples/RagOllama/RagOllama.csproj` targeting `net10.0` with:
  - `<ProjectReference>` to `../../src/LocalEmbeddings/LocalEmbeddings.csproj`
  - `OllamaSharp` NuGet package (~5.4.16) for `IChatClient` access to phi-3.5-mini
- New `samples/RagOllama/Program.cs` — self-contained top-level statements:
  - Creates `LocalEmbeddingGenerator` for local embeddings
  - Creates `OllamaApiClient` pointing to `http://localhost:11434` with model `phi3.5`
  - Embeds a small set of hardcoded knowledge documents into an in-memory list with cosine similarity search
  - Interactive loop: user query → embed query → find top-K similar docs → build prompt with context → send to phi-3.5-mini via `IChatClient.GetStreamingResponseAsync` → stream response to console
- New `samples/RagOllama/SimpleVectorStore.cs` — minimal self-contained vector store (~50 lines): `AddAsync`, `SearchAsync` using `CosineSimilarity` from `EmbeddingExtensions`
- New `samples/RagOllama/KnowledgeBase.cs` — static class with ~10 sample documents (short tech FAQ about .NET embedding generation)

### 2. Create `samples/RagFoundryLocal/` project

- New `samples/RagFoundryLocal/RagFoundryLocal.csproj` targeting `net10.0` with:
  - `<ProjectReference>` to `../../src/LocalEmbeddings/LocalEmbeddings.csproj`
  - `Microsoft.AI.Foundry.Local` NuGet package (~0.8.2) for local model management
  - `Microsoft.Extensions.AI.OpenAI` NuGet package for `IChatClient` over OpenAI-compatible endpoint
- New `samples/RagFoundryLocal/Program.cs` — same RAG pattern as Ollama sample but:
  - Uses `FoundryLocalManager` to start phi-3.5-mini locally
  - Creates `OpenAIClient` pointing to `manager.Endpoint` with `manager.ApiKey`
  - Wraps it as `IChatClient` via `Microsoft.Extensions.AI.OpenAI` helpers
  - Same interactive RAG loop
- New `samples/RagFoundryLocal/SimpleVectorStore.cs` — same standalone vector store as Ollama sample
- New `samples/RagFoundryLocal/KnowledgeBase.cs` — same knowledge base (shared content, separate file)

### 3. Create `samples/README.md`

- Overview of all 4 samples in a table: ConsoleApp, RagChat, RagOllama, RagFoundryLocal
- Each entry: name, description, prerequisites (e.g., "Ollama running with phi3.5 pulled", "Foundry Local installed"), and a link to the sample folder
- Quick "how to run" instructions for each

### 4. Add solution entries

- Update `LocalEmbeddings.slnx` to include both new projects and the existing RagChat (currently missing)

### 5. Slim down `README.md`

Restructure the root README to keep only:

- Title + badges
- One-paragraph description
- Installation (dotnet CLI only)
- Quick Start — single minimal "hello world" code block (create generator, generate embedding, print dimension)
- **Samples** — brief list linking to `samples/README.md` and each sample folder
- **Documentation** — links to docs: API Reference, Configuration, Contributing, Publishing
- Building from Source (clone + build + test — 3 commands)
- Requirements + License (kept short)

### 6. Move detailed content to `docs/`

- Update `docs/api-reference.md` — add missing `NormalizeEmbeddings` option, add `EmbeddingExtensions` methods (`CosineSimilarity`, `FindClosest`), add `CreateAsync` factory method
- Update `docs/configuration.md` — absorb the DI section, Supported Models section, and Cache Locations section from the current README
- Create new `docs/dependency-injection.md` — detailed DI guide with all 4 overloads, `IConfiguration` binding, and `IEmbeddingGenerator` injection patterns (content currently in README)

## Verification

- `dotnet build` at solution level should compile all projects (including new samples) with zero errors/warnings
- `dotnet run --project samples/RagOllama` with Ollama running + phi3.5 pulled should start the interactive RAG chat
- `dotnet run --project samples/RagFoundryLocal` with Foundry Local installed should start the interactive RAG chat
- Verify all markdown links resolve correctly — cross-check README → docs/ → samples/ links
- Verify the NuGet README still renders the essential info (it packs `README.md` from root)

## Decisions

| Decision | Rationale |
|----------|-----------|
| **OllamaSharp over Microsoft.Extensions.AI.Ollama** | The Microsoft package is deprecated; OllamaSharp is the recommended replacement and implements `IChatClient` natively |
| **Foundry Local + OpenAI client** | Foundry Local exposes an OpenAI-compatible endpoint, so we use `Microsoft.Extensions.AI.OpenAI` to get `IChatClient` — no custom integration needed |
| **Standalone samples** | Each RAG sample is fully self-contained (own vector store, own data) so users can copy one folder and run it independently |
| **Keep RagChat** | It remains as a simpler embedding-search-only example, showing LocalEmbeddings without requiring an LLM |
