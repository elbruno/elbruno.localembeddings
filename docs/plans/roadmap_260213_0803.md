# ElBruno.LocalEmbeddings — Roadmap & Feature Proposals

**Created:** 2026-02-13 08:03  
**Status:** Draft — ready for implementation planning

---

## Table of Contents

1. [Library Improvements](#library-improvements)
   - [1. Multi-TFM Support (net8.0 + net10.0)](#1-multi-tfm-support-net80--net100)
   - [2. SIMD / TensorPrimitives for Similarity & Normalization](#2-simd--tensorprimitives-for-similarity--normalization)
   - [3. GPU / Hardware Acceleration (DirectML, CUDA)](#3-gpu--hardware-acceleration-directml-cuda)
   - [4. Microsoft.Extensions.VectorData Integration](#4-microsoftextensionsvectordata-integration)
   - [5. Embedding Caching Middleware](#5-embedding-caching-middleware)
   - [6. True Async Model Loading](#6-true-async-model-loading)
   - [7. Quantized Model Support (INT8 / ONNX QDQ)](#7-quantized-model-support-int8--onnx-qdq)
   - [8. Model Integrity Verification (SHA256)](#8-model-integrity-verification-sha256)
   - [15. Single-String Convenience API (DX)](#15-single-string-convenience-api-dx)

- [16. Cross-Platform Native Runtime Reliability (Linux ARM64)](#16-cross-platform-native-runtime-reliability-linux-arm64)
- [18. Dev Container Support (VS Code)](#18-dev-container-support-vs-code)

2. [New Samples](#new-samples)

- [17. RaspberryPiTiny — Ultra-Small Stability Sample](#17-raspberrypitiny--ultra-small-stability-sample)
- [9. VectorDataSample — Microsoft.Extensions.VectorData](#9-vectordatasample--microsoftextensionsvectordata)
- [10. SemanticKernelSample — SK Plugin with Local Embeddings](#10-semantickernelsample--sk-plugin-with-local-embeddings)
- [11. AspNetApiSample — Embedding Microservice](#11-aspnetapisample--embedding-microservice)
- [12. BenchmarkSample — BenchmarkDotNet Performance Suite](#12-benchmarksample--benchmarkdotnet-performance-suite)
- [13. ImageSearchSample — Multimodal CLIP Search](#13-imagesearchsample--multimodal-clip-search)
- [14. MAUISample — Desktop / Mobile Semantic Note Search](#14-mauisample--desktop--mobile-semantic-note-search)

3. [Priority Matrix](#priority-matrix)

---

## Library Improvements

### 1. Multi-TFM Support (net8.0 + net10.0) ✅ Done

**Priority:** High | **Effort:** Low | **Impact:** Very High

**Problem:** The library targets only `net10.0`. .NET 8 is the current LTS release and the most widely deployed runtime.

**Proposal:** Add `net8.0` as an additional target framework in both `ElBruno.LocalEmbeddings` and `ElBruno.LocalEmbeddings.KernelMemory`.

**Implementation details (completed):**

- [x] **File:** `src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj`
  - Changed `<TargetFramework>net10.0</TargetFramework>` → `<TargetFrameworks>net8.0;net10.0</TargetFrameworks>`.
- [x] **File:** `src/ElBruno.LocalEmbeddings.KernelMemory/ElBruno.LocalEmbeddings.KernelMemory.csproj`
  - Applied same TFM change.
- [x] **File:** `tests/ElBruno.LocalEmbeddings.Tests/ElBruno.LocalEmbeddings.Tests.csproj`
  - Targeted both TFMs so tests run on both.
- [x] **File:** `tests/ElBruno.LocalEmbeddings.KernelMemory.Tests/ElBruno.LocalEmbeddings.KernelMemory.Tests.csproj`
  - Applied same TFM change.
- [x] Reviewed `PackageReference` compatibility for net8.0 by validating build/test across both TFMs.
- [x] `#if NET10_0_OR_GREATER` directives were not needed (no API differences encountered).
- [x] `Directory.Build.props` update was not required.
- [x] Samples remain on `net10.0` (no sample TFM changes made).

---

### 2. SIMD / TensorPrimitives for Similarity & Normalization

**Priority:** High | **Effort:** Low | **Impact:** Medium

**Problem:** `EmbeddingExtensions.CosineSimilarity` and `OnnxEmbeddingModel.L2Normalize` use scalar `for` loops. For high-dimensional vectors (384+ floats), SIMD provides 4-8x speedups.

**Proposal:** Replace scalar math with `System.Numerics.Tensors.TensorPrimitives` (available since .NET 8).

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingExtensions.cs`
  - Replace `CosineSimilarity(ReadOnlyMemory<float>, ReadOnlyMemory<float>)` body:

    ```csharp
    // Before (scalar loop):
    float dotProduct = 0; float normA = 0; float normB = 0;
    for (int i = 0; i < spanA.Length; i++) { ... }

    // After:
    return TensorPrimitives.CosineSimilarity(spanA, spanB);
    ```

  - Add `using System.Numerics.Tensors;`

- **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs`
  - Replace `L2Normalize(float[])` body:

    ```csharp
    // Before:
    var norm = MathF.Sqrt(vector.Sum(x => x * x));
    if (norm > 0) { for (int i ...) vector[i] /= norm; }

    // After:
    var norm = TensorPrimitives.Norm(vector);
    if (norm > 0)
    {
        TensorPrimitives.Divide(vector, norm, vector);
    }
    ```

  - Add `using System.Numerics.Tensors;`

- **NuGet dependency:** Add `<PackageReference Include="System.Numerics.Tensors" Version="9.0.3" />` (or latest). On .NET 8+, `TensorPrimitives` is inbox or available as a package.

- If multi-TFM is implemented first, `TensorPrimitives` is available on both net8.0 and net10.0. No conditional compilation needed.

- **Tests:** Existing `LocalEmbeddingGeneratorTests` should cover this since the behavior is unchanged; add a dedicated cosine similarity unit test comparing known vectors if not already present.

---

### 3. GPU / Hardware Acceleration (DirectML, CUDA)

**Priority:** Medium | **Effort:** Medium | **Impact:** High

**Problem:** The library uses `Microsoft.ML.OnnxRuntime` (CPU-only). Users with GPUs get no acceleration benefit.

**Proposal:** Add an `ExecutionProvider` enum option and companion NuGet packages for GPU execution providers.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Options/ExecutionProviderType.cs`

  ```csharp
  namespace ElBruno.LocalEmbeddings.Options;

  public enum ExecutionProviderType
  {
      Cpu,
      DirectML,    // Windows GPU via DirectX
      Cuda,        // NVIDIA GPU
      CoreML       // macOS Apple Silicon
  }
  ```

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`
  - Add property:

    ```csharp
    public ExecutionProviderType ExecutionProvider { get; set; } = ExecutionProviderType.Cpu;
    ```

  - Add optional GPU device ID:

    ```csharp
    public int DeviceId { get; set; } = 0;
    ```

- **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs` — `Load()` method
  - After creating `SessionOptions`, append execution provider before creating `InferenceSession`:

    ```csharp
    switch (executionProvider)
    {
        case ExecutionProviderType.DirectML:
            sessionOptions.AppendExecutionProvider_DML(deviceId);
            break;
        case ExecutionProviderType.Cuda:
            sessionOptions.AppendExecutionProvider_CUDA(deviceId);
            break;
        case ExecutionProviderType.CoreML:
            sessionOptions.AppendExecutionProvider_CoreML();
            break;
    }
    ```

  - The `Load()` method signature needs to accept `ExecutionProviderType` and `int deviceId` parameters (or receive the full options object).

- **Companion NuGet packages (optional, future):**
  - `ElBruno.LocalEmbeddings.DirectML` — depends on `Microsoft.ML.OnnxRuntime.DirectML`
  - `ElBruno.LocalEmbeddings.Cuda` — depends on `Microsoft.ML.OnnxRuntime.Gpu`
  - These would be thin metapackages that pull in the right native binaries.

- **Alternative simpler approach:** Document that users can swap out the `Microsoft.ML.OnnxRuntime` NuGet for `Microsoft.ML.OnnxRuntime.DirectML` or `Microsoft.ML.OnnxRuntime.Gpu` in their project, and the same `SessionOptions` API works. The library just needs to expose the `ExecutionProvider` option to configure the session.

- **Testing:** GPU tests should use `[SkippableFact]` and check for GPU availability at runtime.

---

### 4. Microsoft.Extensions.VectorData Integration

**Priority:** High (ecosystem alignment) | **Effort:** Medium | **Impact:** High

**Problem:** The library includes a custom `InMemoryVectorStore` in the RagChat sample and custom `FindClosest` extensions. The .NET ecosystem is converging on `Microsoft.Extensions.VectorData.Abstractions` (v9.7.0) as the standard for vector stores.

**Proposal:** Create a companion package `ElBruno.LocalEmbeddings.VectorData` that provides integration with the `Microsoft.Extensions.VectorData` ecosystem.

**Implementation details:**

- **New project:** `src/ElBruno.LocalEmbeddings.VectorData/ElBruno.LocalEmbeddings.VectorData.csproj`

  ```xml
  <PackageReference Include="Microsoft.Extensions.VectorData.Abstractions" Version="9.7.0" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  ```

- **New file:** `src/ElBruno.LocalEmbeddings.VectorData/Extensions/ServiceCollectionExtensions.cs`
  - Extension method `AddLocalEmbeddingsWithVectorStore()` that registers:
    - `IEmbeddingGenerator<string, Embedding<float>>` (the existing local generator)
    - `VectorStore` (from `Microsoft.Extensions.VectorData`)
  - Example usage:

    ```csharp
    services.AddLocalEmbeddings(o => { o.ModelName = "..."; })
            .AddInMemoryVectorStore();  // or .AddSqliteVecStore(), etc.
    ```

- **Key types from M.E.VectorData to integrate with:**
  - `VectorStoreCollection<TKey, TRecord>` — for CRUD + search
  - `VectorStoreDataAttribute`, `VectorStoreKeyAttribute`, `VectorStoreVectorAttribute` — record decorators
  - `IVectorSearchable<TRecord>` — search interface
  - `VectorSearchOptions<TRecord>` and `VectorSearchResult<TRecord>` — search options/results

- **Migration path:** Update the `RagChat` sample to optionally use `VectorStoreCollection` instead of the custom `InMemoryVectorStore`.

---

### 5. Embedding Caching Middleware

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium

**Problem:** Repeated calls to `GenerateAsync` with the same input text re-run ONNX inference. In scenarios like RAG pipelines, the same documents are often re-embedded.

**Proposal:** Provide a caching `IEmbeddingGenerator` wrapper using the `Microsoft.Extensions.AI` middleware pattern.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Middleware/CachingEmbeddingGenerator.cs`
  - Wraps an `IEmbeddingGenerator<string, Embedding<float>>`
  - Uses `IDistributedCache` or `IMemoryCache` as the backing store
  - Cache key = SHA256 hash of the input text + model name
  - Implements `IEmbeddingGenerator<string, Embedding<float>>` (delegating handler pattern)

- **New file:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingGeneratorBuilderExtensions.cs`

  ```csharp
  public static IServiceCollection AddLocalEmbeddingsWithCaching(
      this IServiceCollection services,
      Action<LocalEmbeddingsOptions>? configure = null)
  {
      services.AddLocalEmbeddings(configure);
      services.AddMemoryCache();
      services.Decorate<IEmbeddingGenerator<string, Embedding<float>>, CachingEmbeddingGenerator>();
      return services;
  }
  ```

- **Alternative:** If `Microsoft.Extensions.AI` already provides `.UseDistributedCache()` for embeddings (like it does for `IChatClient`), then simply document how to use that instead of building a custom one. Check `Microsoft.Extensions.AI` v10.3.0 for embedding caching support.

---

### 6. True Async Model Loading

**Priority:** Medium | **Effort:** Low | **Impact:** Medium

**Problem:** The constructor calls `.GetAwaiter().GetResult()` for model download, which blocks the calling thread. The `CreateAsync` factory wraps this in `Task.Run`, which consumes a thread-pool thread.

**Proposal:** Make model download truly async by restructuring initialization.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Make the constructor accept a pre-resolved `modelDirectory` (no download logic).
  - Move download logic entirely into `CreateAsync`:

    ```csharp
    public static async Task<LocalEmbeddingGenerator> CreateAsync(
        LocalEmbeddingsOptions options,
        CancellationToken cancellationToken = default)
    {
        string modelDirectory;
        if (!string.IsNullOrWhiteSpace(options.ModelPath))
        {
            modelDirectory = options.ModelPath;
        }
        else if (options.EnsureModelDownloaded)
        {
            var downloader = new ModelDownloader(new HttpClient(), options.CacheDirectory);
            modelDirectory = await downloader.EnsureModelAsync(options.ModelName, cancellationToken: cancellationToken);
        }
        else { throw ... }

        return new LocalEmbeddingGenerator(modelDirectory, options);
    }
    ```

  - Add a private/internal constructor that takes `modelDirectory` + options.
  - Keep the existing public constructor for backwards compatibility (it can still do sync download for simple console app scenarios).

- **File:** `src/ElBruno.LocalEmbeddings/Extensions/ServiceCollectionExtensions.cs`
  - The DI registration factory is synchronous. Consider using `IHostedService` or lazy initialization pattern:

    ```csharp
    services.TryAddSingleton<IEmbeddingGenerator<string, Embedding<float>>>(sp =>
    {
        var options = sp.GetRequiredService<IOptions<LocalEmbeddingsOptions>>().Value;
        // Still sync here (DI factory), but at least the CreateAsync path exists for manual use
        return new LocalEmbeddingGenerator(options);
    });
    ```

  - Alternatively, register as `Lazy<Task<LocalEmbeddingGenerator>>` or use an `IHostedService` that pre-warms the model on startup.

---

### 7. Quantized Model Support (INT8 / ONNX QDQ)

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium

**Problem:** Full-precision FP32 ONNX models use ~90 MB for all-MiniLM-L6-v2. INT8 quantized variants use ~23 MB with minimal quality loss.

**Proposal:** Support downloading and loading quantized model variants.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`

  ```csharp
  /// <summary>
  /// Gets or sets whether to prefer a quantized (INT8) model variant if available.
  /// Default is false (uses full-precision FP32 model).
  /// </summary>
  public bool PreferQuantized { get; set; } = false;
  ```

- **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - In `EnsureModelAsync`, when `PreferQuantized` is true, try downloading from:

    ```
    https://huggingface.co/{modelName}/resolve/main/onnx/model_quantized.onnx
    ```

    If that 404s, fall back to the regular `model.onnx`.
  - The quantized model file is typically named `model_quantized.onnx` or `model_int8.onnx` on HuggingFace. Try both names.

- **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - When loading, check for `model_quantized.onnx` first if `PreferQuantized` is set, fall back to `model.onnx`.

- **Documentation:** Add a section in `docs/configuration.md` explaining the trade-offs (file size, memory, speed vs. quality).

---

### 8. Model Integrity Verification (SHA256)

**Priority:** Lower | **Effort:** Low | **Impact:** Low

**Problem:** Downloaded model files are not verified for integrity. Corrupted downloads or MITM attacks could lead to loading tampered models.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - After downloading a file, compute SHA256 and compare against expected hash.
  - HuggingFace Hub API provides file hashes at:

    ```
    GET https://huggingface.co/api/models/{modelName}/tree/main
    ```

    Response includes `{ "oid": "<sha256>", "size": <bytes> }` for each file.
  - Add optional `VerifyIntegrity` property to `LocalEmbeddingsOptions` (default `false` to avoid breaking changes, or `true` for secure-by-default).
  - On hash mismatch, delete the corrupted file and throw `InvalidOperationException`.

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`

  ```csharp
  public bool VerifyModelIntegrity { get; set; } = false;
  ```

---

### 15. Single-String Convenience API (DX)

**Priority:** High | **Effort:** Low | **Impact:** High | **Status:** ✅ Completed (2026-02-13)  
**Plan:** [plan_260213_0913.md](plan_260213_0913.md)

**Problem:** Every caller must wrap a single string in a collection (`["text"]`) and index the result with `[0]`, e.g. `await generator.GenerateAsync(["Hello"]); result[0].Vector`. This boilerplate hurts discoverability and first-run experience.

**Proposal:** Add two extension methods on `IEmbeddingGenerator<string, Embedding<float>>` in the `ElBruno.LocalEmbeddings` namespace:

| Method | Returns | Description |
|--------|---------|-------------|
| `GenerateAsync(string)` | `Task<GeneratedEmbeddings<Embedding<float>>>` | Wraps the single string in a collection and delegates |
| `GenerateEmbeddingAsync(string)` | `Task<Embedding<float>>` | Returns a single `Embedding<float>` directly — no indexing needed |

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/EmbeddingGeneratorExtensions.cs`
  - `public static class EmbeddingGeneratorExtensions` in `ElBruno.LocalEmbeddings` namespace
  - Both methods validate arguments with `ArgumentNullException.ThrowIfNull`
  - Full XML doc comments with `<example>` blocks

- **Tests:** 7 new tests in `tests/ElBruno.LocalEmbeddings.Tests/LocalEmbeddingGeneratorTests.cs`
  - Unit tests for null-guard on generator and value
  - Integration tests (SkippableFact) for correct dimensions, result equality with batch API

- **Samples:** Update all samples (`ConsoleApp`, `HelloWorldAltModel`, `RagChat`, etc.) to use the simpler API for single-text calls

- **Docs:** Update `README.md` Quick Start, `docs/api-reference.md`, `docs/getting-started.md` to showcase the convenience API

- **Why extension methods on the interface?** Works through DI where users receive `IEmbeddingGenerator<string, Embedding<float>>`, not the concrete `LocalEmbeddingGenerator`.

- **Why main namespace?** Methods appear in IntelliSense without extra `using` statements — best DX.

**Before:**

```csharp
var result = await generator.GenerateAsync(["Hello, world!"]);
float[] embedding = result[0].Vector.ToArray();
```

**After:**

```csharp
var embedding = await generator.GenerateEmbeddingAsync("Hello, world!");
float[] vector = embedding.Vector.ToArray();
```

---

### 16. Cross-Platform Native Runtime Reliability (Linux ARM64)

**Priority:** High | **Effort:** Low-Medium | **Impact:** High  
**Status:** Planned  
**Plan:** [plan_260213_1130.md](plan_260213_1130.md)

**Problem:** `samples/ConsoleApp` can fail on Linux ARM64 with ONNX Runtime native library load errors (`DllNotFoundException`) while Windows scenarios continue to work.

**Proposal:** Add a minimal Linux-only compatibility shim around ONNX Runtime native loading to preserve Windows behavior and improve cross-platform reliability.

**Implementation details:**

- Add Linux-only native load compatibility handling in `OnnxEmbeddingModel` initialization path.
- Keep Windows/macOS path unchanged and avoid API changes.
- Improve diagnostics when native loading fails (OS/arch + expected native artifacts).
- Validate through build/tests plus sample execution on Linux ARM64.

**Fallback strategy:**

- If the fix grows too platform-specific for core code, keep the canonical `ConsoleApp` and add a Linux-focused sample variant with explicit runtime guidance.

---

### 18. Dev Container Support (VS Code)

**Priority:** High | **Effort:** Low | **Impact:** High  
**Status:** Planned  
**Plan:** [plan_260213_1946.md](plan_260213_1946.md)

**Problem:** Contributors need local SDK/runtime setup, and environment drift can cause onboarding friction and inconsistent build/test behavior across machines.

**Proposal:** Add a repository-level VS Code Dev Container using .NET 10, with persistent NuGet and model caches and explicit contributor documentation.

**Implementation details:**

- **New file:** `.devcontainer/devcontainer.json`
  - Base image aligned with CI SDK (`10.0.x` lineage).
  - Workspace at repository root.
  - `postCreateCommand` to run `dotnet restore`.
  - Recommended extensions for C#/.NET developer workflow.
  - Named mounts/volumes for:
    - NuGet global package cache.
    - Local model cache used by runtime model downloads.

- **Docs updates:**
  - **File:** `docs/contributing.md`
    - Add Dev Container usage section (prerequisites, reopen flow, first-run model download note, validation sequence).

- **Validation:**
  - Run `dotnet restore`, `dotnet build`, and `dotnet test` from repository root in-container.

**Expected benefits:**

- Faster onboarding with zero local SDK setup for contributors.
- Better parity with CI/Linux behavior.
- Reduced repeated download costs via persistent caches.

---

## New Samples

### 17. RaspberryPiTiny — Ultra-Small Stability Sample

**Priority:** High | **Effort:** Low | **Impact:** High | **Status:** ✅ Completed (2026-02-13)

**Purpose:** Provide a minimal sample designed for low-resource devices (for example Raspberry Pi) where the full multi-scenario console sample can trigger device instability.

**Location:** `samples/RaspberryPiTiny/`

**What it does:**

- Default mode runs one embedding generation and exits quickly
- Optional `sim` mode runs two short embeddings and a cosine similarity check
- Uses conservative ONNX runtime settings (`ORT_SEQUENTIAL`, single thread)

**Key files:**

- `samples/RaspberryPiTiny/RaspberryPiTiny.csproj`
- `samples/RaspberryPiTiny/Program.cs`

---

### 9. VectorDataSample — Microsoft.Extensions.VectorData

**Priority:** High | **Effort:** Medium | **Impact:** High

**Purpose:** Show how `ElBruno.LocalEmbeddings` works with the official `Microsoft.Extensions.VectorData` abstractions, the pattern Microsoft promotes for vector search in .NET.

**Location:** `samples/VectorDataSample/`

**Key files:**

- `VectorDataSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.Extensions.VectorData.Abstractions" Version="9.7.0" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `CloudService.cs` — data model decorated with VectorData attributes:

  ```csharp
  internal class CloudService
  {
      [VectorStoreKey]
      public int Key { get; set; }

      [VectorStoreData]
      public string Name { get; set; }

      [VectorStoreData]
      public string Description { get; set; }

      [VectorStoreVector(Dimensions: 384, DistanceFunction = DistanceFunction.CosineSimilarity)]
      public ReadOnlyMemory<float> Vector { get; set; }
  }
  ```

- `Program.cs` — flow:
  1. Create `LocalEmbeddingGenerator` as `IEmbeddingGenerator<string, Embedding<float>>`
  2. Create `InMemoryVectorStore` from Semantic Kernel connectors
  3. Get a `VectorStoreCollection<int, CloudService>`
  4. Embed sample data and upsert into the collection
  5. Accept user queries, embed them, perform `SearchAsync`, display results

**What this demonstrates:**

- Drop-in compatibility with the standard .NET vector store ecosystem
- Users can swap `InMemory` for Qdrant, Azure AI Search, Cosmos DB, pgvector, etc. by changing one line

---

### 10. SemanticKernelSample — SK Plugin with Local Embeddings

**Priority:** Medium | **Effort:** Low | **Impact:** High

**Purpose:** Demonstrate Semantic Kernel with local embeddings for a fully offline AI assistant (Ollama for chat + local embeddings for memory).

**Location:** `samples/SemanticKernelSample/`

**Key files:**

- `SemanticKernelSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.SemanticKernel" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.Ollama" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs` — flow:
  1. Build a Semantic Kernel with Ollama chat completion
  2. Create `LocalEmbeddingGenerator` and register as the embedding generator with SK
  3. Create an in-memory vector store for semantic memory
  4. Import sample documents into memory
  5. Interactive chat loop where the kernel retrieves relevant memory before answering
  6. All running 100% locally — no API keys needed

**What this demonstrates:**

- Full Semantic Kernel integration
- Offline-first RAG pattern
- How `IEmbeddingGenerator` from M.E.AI plugs into SK's memory system

---

### 11. AspNetApiSample — Embedding Microservice

**Priority:** High | **Effort:** Low | **Impact:** High

**Purpose:** A minimal ASP.NET Core API exposing embedding generation as an HTTP endpoint, showcasing DI, health checks, and production patterns.

**Location:** `samples/AspNetApiSample/`

**Key files:**

- `AspNetApiSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.AspNetCore.OpenApi" Version="10.0.0" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs`:

  ```csharp
  var builder = WebApplication.CreateBuilder(args);

  builder.Services.AddLocalEmbeddings(options =>
  {
      options.ModelName = "sentence-transformers/all-MiniLM-L6-v2";
      options.EnsureModelDownloaded = true;
  });

  builder.Services.AddOpenApi();

  var app = builder.Build();

  app.MapOpenApi();

  app.MapPost("/api/embeddings", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      EmbedRequest request) =>
  {
      var result = await generator.GenerateAsync(request.Texts);
      return Results.Ok(new EmbedResponse
      {
          Embeddings = result.Select(e => e.Vector.ToArray()).ToList(),
          Model = generator.Metadata.DefaultModelId,
          Dimensions = generator.Metadata.DefaultModelDimensions
      });
  });

  app.MapPost("/api/similarity", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      SimilarityRequest request) =>
  {
      var embeddings = await generator.GenerateAsync([request.Text1, request.Text2]);
      var similarity = embeddings[0].CosineSimilarity(embeddings[1]);
      return Results.Ok(new { Similarity = similarity });
  });

  app.MapGet("/health", () => Results.Ok("healthy"));

  app.Run();
  ```

- `Models.cs` — request/response DTOs:

  ```csharp
  record EmbedRequest(string[] Texts);
  record EmbedResponse { List<float[]> Embeddings; string Model; int? Dimensions; }
  record SimilarityRequest(string Text1, string Text2);
  ```

**What this demonstrates:**

- Production DI registration pattern
- REST API compatible with OpenAI embeddings format
- Health endpoint for container orchestration
- OpenAPI/Swagger documentation

---

### 12. BenchmarkSample — BenchmarkDotNet Performance Suite

**Priority:** Medium | **Effort:** Low | **Impact:** Medium

**Purpose:** Provide reproducible performance benchmarks for different configurations, useful for documentation and regression testing.

**Location:** `samples/BenchmarkSample/`

**Key files:**

- `BenchmarkSample.csproj`:

  ```xml
  <PackageReference Include="BenchmarkDotNet" Version="0.14.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `EmbeddingBenchmarks.cs`:

  ```csharp
  [MemoryDiagnoser]
  [SimpleJob(RuntimeMoniker.Net100)]
  public class EmbeddingBenchmarks
  {
      private LocalEmbeddingGenerator _generator;
      private string[] _singleText;
      private string[] _batchTexts;

      [GlobalSetup]
      public void Setup()
      {
          _generator = new LocalEmbeddingGenerator(new LocalEmbeddingsOptions
          {
              ModelName = "sentence-transformers/all-MiniLM-L6-v2"
          });
          _singleText = ["The quick brown fox jumps over the lazy dog."];
          _batchTexts = Enumerable.Range(0, 100)
              .Select(i => $"Sample sentence number {i} for benchmarking embeddings.")
              .ToArray();
      }

      [Benchmark(Baseline = true)]
      public Task<GeneratedEmbeddings<Embedding<float>>> SingleEmbedding()
          => _generator.GenerateAsync(_singleText);

      [Benchmark]
      [Arguments(10)]
      [Arguments(50)]
      [Arguments(100)]
      public Task<GeneratedEmbeddings<Embedding<float>>> BatchEmbedding(int count)
          => _generator.GenerateAsync(_batchTexts.Take(count));

      [GlobalCleanup]
      public void Cleanup() => _generator.Dispose();
  }
  ```

- `SimilarityBenchmarks.cs` — benchmarks cosine similarity (scalar vs. TensorPrimitives, before/after the SIMD improvement).

**What this demonstrates:**

- Throughput for single vs. batch inference
- Memory allocation profiles
- Baseline numbers for the README / docs

---

### 13. ImageSearchSample — Multimodal CLIP Search

**Priority:** Lower | **Effort:** High | **Impact:** Medium

**Purpose:** Demonstrate text-to-image semantic search using CLIP-style models, all running locally.

**Location:** `samples/ImageSearchSample/`

**Prerequisites:**

- A CLIP model exported to ONNX (e.g., `openai/clip-vit-base-patch32` from HuggingFace, ONNX export)
- CLIP uses separate text and image encoders; the library would need to support loading two ONNX models or the sample handles image encoding separately

**Flow:**

1. Load a folder of images
2. Generate image embeddings using the CLIP vision encoder ONNX model
3. Accept natural language queries
4. Generate text embeddings using the CLIP text encoder
5. Compute cosine similarity between text embedding and all image embeddings
6. Display top-K matching images

**Note:** This sample may require library changes to support non-BERT tokenizers (CLIP uses a BPE tokenizer, not WordPiece). This could motivate a tokenizer abstraction or a separate `ClipEmbeddingGenerator` class. Consider this a stretch goal.

---

### 14. MAUISample — Desktop / Mobile Semantic Note Search

**Priority:** Lower | **Effort:** Medium | **Impact:** Medium

**Purpose:** Show the library working in a .NET MAUI app (Windows, macOS, potentially Android/iOS), proving it's not server-only.

**Location:** `samples/MAUISample/`

**Flow:**

1. User types notes into a text editor
2. Notes are embedded and stored locally (SQLite + embeddings)
3. User can semantically search their notes ("find my notes about cooking recipes")
4. Results ranked by cosine similarity

**Implementation considerations:**

- ONNX Runtime supports Android (via NNAPI) and iOS (via CoreML), but the NuGet packages differ
- The model download should happen on first launch with a progress indicator
- Use `IEmbeddingGenerator<string, Embedding<float>>` via DI in MAUI's `MauiProgram.cs`

---

## Priority Matrix

| Priority | Item | Effort | Impact | Dependencies |
|----------|------|--------|--------|-------------|
| **High** | 1. Multi-TFM (net8.0 + net10.0) | Low | Very High | None | ✅ Done |
| **High** | 2. SIMD/TensorPrimitives | Low | Medium | None (or after #1) |
| **High** | 9. VectorData sample | Medium | High | None (uses existing API) |
| **High** | 11. ASP.NET API sample | Low | High | None |
| **High** | 15. Single-String Convenience API | Low | High | None | ✅ Done |
| **High** | 16. Cross-platform native runtime reliability | Low-Medium | High | None |
| **High** | 18. Dev Container support (VS Code) | Low | High | None |
| **High** | 17. RaspberryPiTiny sample | Low | High | #16 helpful | ✅ Done |
| **Medium** | 3. GPU acceleration | Medium | High | None |
| **Medium** | 4. VectorData companion package | Medium | High | After #9 (sample validates the design) |
| **Medium** | 5. Caching middleware | Medium | Medium | Check M.E.AI built-in support first |
| **Medium** | 6. Async model loading | Low | Medium | None |
| **Medium** | 7. Quantized model support | Medium | Medium | None |
| **Medium** | 10. Semantic Kernel sample | Low | High | None |
| **Medium** | 12. Benchmark sample | Low | Medium | After #2 (to compare before/after SIMD) |
| **Lower** | 8. Model integrity (SHA256) | Low | Low | None |
| **Lower** | 13. Image search (CLIP) | High | Medium | Library tokenizer abstraction needed |
| **Lower** | 14. MAUI sample | Medium | Medium | Multi-TFM (#1) helpful |

### Suggested implementation order

1. **Phase 1 — Quick wins:**
   - ~~#15 Single-String Convenience API (DX)~~ ✅

- #16 Cross-platform native runtime reliability
- #18 Dev Container support (VS Code)
- ~~#1 Multi-TFM support~~ ✅
- #2 SIMD/TensorPrimitives
- #11 ASP.NET API sample
- #9 VectorData sample

1. **Phase 2 — Ecosystem integration:**
   - #4 VectorData companion package
   - #10 Semantic Kernel sample
   - #6 Async model loading
   - #12 Benchmark sample

2. **Phase 3 — Advanced features:**
   - #3 GPU acceleration
   - #7 Quantized model support
   - #5 Caching middleware
   - #8 Model integrity verification

3. **Phase 4 — Stretch goals:**
   - #13 Image search (CLIP)
   - #14 MAUI sample
