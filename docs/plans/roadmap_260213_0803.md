# ElBruno.LocalEmbeddings — Roadmap & Feature Proposals

**Created:** 2026-02-13 08:03  
**Status:** Draft — ready for implementation planning

---

## Table of Contents

1. [Library Improvements](#library-improvements) *(alphabetical)*

- [19. ✅ Issue #24 Reliability, DX & Test Coverage Sweep](#19--issue-24-reliability-dx--test-coverage-sweep)
- [16. Cross-Platform Native Runtime Reliability (Linux ARM64)](#16-cross-platform-native-runtime-reliability-linux-arm64)
- [18. ✅ Dev Container Support (VS Code)](#18--dev-container-support-vs-code)
- [5. Embedding Caching Middleware](#5-embedding-caching-middleware)
- [3. GPU / Hardware Acceleration (DirectML, CUDA)](#3-gpu--hardware-acceleration-directml-cuda)
- [8. Model Integrity Verification (SHA256)](#8-model-integrity-verification-sha256)
- [4. ✅ Microsoft.Extensions.VectorData Integration](#4--microsoftextensionsvectordata-integration)
- [1. ✅ Multi-TFM Support (net8.0 + net10.0)](#1--multi-tfm-support-net80--net100)
- [7. ✅ Quantized Model Support (INT8 / ONNX QDQ)](#7--quantized-model-support-int8--onnx-qdq)
- [2. ✅ SIMD / TensorPrimitives for Similarity & Normalization](#2--simd--tensorprimitives-for-similarity--normalization)
- [15. ✅ Single-String Convenience API (DX)](#15--single-string-convenience-api-dx)
- [6. ✅ True Async Model Loading](#6--true-async-model-loading)

1. [New Samples](#new-samples) *(alphabetical)*
   - [11. AspNetApiSample — Embedding Microservice](#11-aspnetapisample--embedding-microservice)
   - [12. ✅ BenchmarkSample — BenchmarkDotNet Performance Suite](#12--benchmarksample--benchmarkdotnet-performance-suite)
   - [13. ImageSearchSample — Multimodal CLIP Search](#13-imagesearchsample--multimodal-clip-search)
   - [14. MAUISample — Desktop / Mobile Semantic Note Search](#14-mauisample--desktop--mobile-semantic-note-search)
   - [17. ✅ RaspberryPiTiny — Ultra-Small Stability Sample](#17--raspberrypitiny--ultra-small-stability-sample)
   - [10. SemanticKernelSample — SK Plugin with Local Embeddings](#10-semantickernelsample--sk-plugin-with-local-embeddings)
   - [9. VectorDataSample — Microsoft.Extensions.VectorData](#9-vectordatasample--microsoftextensionsvectordata)

2. [Priority Matrix](#priority-matrix)

---

## Library Improvements

### 19. ✅ Issue #24 Reliability, DX & Test Coverage Sweep

**Priority:** High | **Effort:** Medium | **Impact:** High | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_1021.md](plan_260214_1021.md)

**Problem:** Issue #24 identified resource-lifetime risks, missing ergonomics for search-only Kernel Memory usage, limited cancellation propagation, and test/documentation gaps.

**Implementation details (completed):**

- [x] **Resource lifetime fix**
  - **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Replaced ad-hoc `new HttpClient()` allocations in model resolution paths with a reusable shared `HttpClient` instance.
- [x] **Async disposal support**
  - **Files:**
    - `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
    - `src/ElBruno.LocalEmbeddings.KernelMemory/LocalEmbeddingTextGenerator.cs`
  - Added `IAsyncDisposable` support while preserving existing `IDisposable` behavior.
- [x] **Cancellation forwarding**
  - **Files:**
    - `src/ElBruno.LocalEmbeddings/Tokenizer.cs`
    - `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs`
    - `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Cancellation now flows through tokenization and batched inference loops.
- [x] **Kernel Memory search-only helper**
  - **File:** `src/ElBruno.LocalEmbeddings.KernelMemory/Extensions/KernelMemoryBuilderExtensions.cs`
  - Added `WithLocalEmbeddingsSearchOnly(...)` overloads that configure `WithoutTextGenerator()` + local embeddings for retrieval-only scenarios.
- [x] **Token counting accuracy improvement**
  - **Files:**
    - `src/ElBruno.LocalEmbeddings/Tokenizer.cs`
    - `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
    - `src/ElBruno.LocalEmbeddings.KernelMemory/LocalEmbeddingTextGenerator.cs`
  - Added tokenizer-backed `CountTokens` and used it automatically when the KM adapter wraps `LocalEmbeddingGenerator`.
- [x] **Test coverage expansion**
  - **New files:**
    - `tests/ElBruno.LocalEmbeddings.KernelMemory.Tests/LocalEmbeddingTextGeneratorTests.cs`
    - `tests/ElBruno.LocalEmbeddings.Tests/ServiceCollectionExtensionsTests.cs`
    - `tests/ElBruno.LocalEmbeddings.Tests/OnnxEmbeddingModelTests.cs`
  - Removed placeholder smoke test from Kernel Memory tests.
- [x] **Docs/changelog updates**
  - **New file:** `docs/changelog.md`
  - Updated docs: `README.md`, `docs/getting-started.md`, `docs/configuration.md`, `docs/api-reference.md`, `docs/dependency-injection.md`, `docs/kernel-memory-integration.md`.

### 1. ✅ Multi-TFM Support (net8.0 + net10.0)

**Priority:** High | **Effort:** Low | **Impact:** Very High | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_2136.md](plan_260214_2136.md)

**Problem:** The library targets only `net10.0`. .NET 8 is the current LTS release and the most widely deployed runtime.

**Proposal:** Add `net8.0` as an additional target framework in both `ElBruno.LocalEmbeddings` and `ElBruno.LocalEmbeddings.KernelMemory`.

**Implementation details (completed):**

- [x] **File:** `src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj`
  - Changed `<TargetFramework>net10.0</TargetFramework>` → `<TargetFrameworks>net8.0;net10.0</TargetFrameworks>`.
- [x] **File:** `src/ElBruno.LocalEmbeddings.KernelMemory/ElBruno.LocalEmbeddings.KernelMemory.csproj`
  - Applied same TFM change.
- [x] **File:** `tests/ElBruno.LocalEmbeddings.Tests/ElBruno.LocalEmbeddings.Tests.csproj`
  - Targeted both TFMs so tests run on both.
- [x] **File:** `tests/ElBruno.LocalEmbeddings.KernelMemory.Tests/ElBruno.LocalEmbeddings.KernelMemory.Tests.csproj`
  - Applied same TFM change.
- [x] Reviewed `PackageReference` compatibility for net8.0 by validating build/test across both TFMs.
- [x] `#if NET10_0_OR_GREATER` directives were not needed (no API differences encountered).
- [x] `Directory.Build.props` update was not required.
- [x] Samples remain on `net10.0` (no sample TFM changes made).

---

### 2. ✅ SIMD / TensorPrimitives for Similarity & Normalization

**Priority:** High | **Effort:** Low | **Impact:** Medium | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_2151.md](plan_260214_2151.md)

**Problem:** `EmbeddingExtensions.CosineSimilarity` and `OnnxEmbeddingModel.L2Normalize` used scalar loops. For high-dimensional vectors (384+ floats), SIMD provides measurable speedups.

**Proposal:** Replace scalar math with `System.Numerics.Tensors.TensorPrimitives` (available on net8.0+).

**Implementation details (completed):**

- [x] **File:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingExtensions.cs`
  - Replaced scalar cosine similarity math with `TensorPrimitives.CosineSimilarity(spanA, spanB)`.
  - Added a `TensorPrimitives` alias import from `System.Numerics.Tensors`.
- [x] **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs`
  - Replaced scalar L2 normalization math with:
    - `TensorPrimitives.Norm(vector)`
    - `TensorPrimitives.Divide(vector, norm, vector)` when `norm > 0`
  - Added a `TensorPrimitives` alias import from `System.Numerics.Tensors`.
- [x] **File:** `src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj`
  - Added `<PackageReference Include="System.Numerics.Tensors" Version="9.0.3" />`.
- [x] **File:** `tests/ElBruno.LocalEmbeddings.Tests/EmbeddingExtensionsTests.cs`
  - Added a known-vectors cosine similarity assertion to confirm behavior remains correct.
- [x] Validated on both TFMs with root `dotnet build` and `dotnet test`.

---

### 3. GPU / Hardware Acceleration (DirectML, CUDA)

**Priority:** Medium | **Effort:** Medium | **Impact:** High

**Problem:** The library uses `Microsoft.ML.OnnxRuntime` (CPU-only). Users with GPUs get no acceleration benefit.

**Proposal:** Add an `ExecutionProvider` enum option and companion NuGet packages for GPU execution providers.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Options/ExecutionProviderType.cs`

  ```csharp
  namespace ElBruno.LocalEmbeddings.Options;

  public enum ExecutionProviderType
  {
      Cpu,
      DirectML,    // Windows GPU via DirectX
      Cuda,        // NVIDIA GPU
      CoreML       // macOS Apple Silicon
  }
  ```

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`
  - Add property:

    ```csharp
    public ExecutionProviderType ExecutionProvider { get; set; } = ExecutionProviderType.Cpu;
    ```

  - Add optional GPU device ID:

    ```csharp
    public int DeviceId { get; set; } = 0;
    ```

- **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs` — `Load()` method
  - After creating `SessionOptions`, append execution provider before creating `InferenceSession`:

    ```csharp
    switch (executionProvider)
    {
        case ExecutionProviderType.DirectML:
            sessionOptions.AppendExecutionProvider_DML(deviceId);
            break;
        case ExecutionProviderType.Cuda:
            sessionOptions.AppendExecutionProvider_CUDA(deviceId);
            break;
        case ExecutionProviderType.CoreML:
            sessionOptions.AppendExecutionProvider_CoreML();
            break;
    }
    ```

  - The `Load()` method signature needs to accept `ExecutionProviderType` and `int deviceId` parameters (or receive the full options object).

- **Companion NuGet packages (optional, future):**
  - `ElBruno.LocalEmbeddings.DirectML` — depends on `Microsoft.ML.OnnxRuntime.DirectML`
  - `ElBruno.LocalEmbeddings.Cuda` — depends on `Microsoft.ML.OnnxRuntime.Gpu`
  - These would be thin metapackages that pull in the right native binaries.

- **Alternative simpler approach:** Document that users can swap out the `Microsoft.ML.OnnxRuntime` NuGet for `Microsoft.ML.OnnxRuntime.DirectML` or `Microsoft.ML.OnnxRuntime.Gpu` in their project, and the same `SessionOptions` API works. The library just needs to expose the `ExecutionProvider` option to configure the session.

- **Testing:** GPU tests should use `[SkippableFact]` and check for GPU availability at runtime.

---

### 4. ✅ Microsoft.Extensions.VectorData Integration

**Priority:** High (ecosystem alignment) | **Effort:** Medium | **Impact:** High
**Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_2214.md](plan_260214_2214.md)

**Problem:** The library includes a custom `InMemoryVectorStore` in the RagChat sample and custom `FindClosest` extensions. The .NET ecosystem is converging on `Microsoft.Extensions.VectorData.Abstractions` (v9.7.0) as the standard for vector stores.

**Proposal:** Create a companion package `ElBruno.LocalEmbeddings.VectorData` that provides integration with the `Microsoft.Extensions.VectorData` ecosystem.

**Implementation details (completed):**

- [x] **New project:** `src/ElBruno.LocalEmbeddings.VectorData/ElBruno.LocalEmbeddings.VectorData.csproj`
  - Added package reference to `Microsoft.Extensions.VectorData.Abstractions` (`9.7.0`).
  - Added package/project metadata consistent with companion packages.
- [x] **New file:** `src/ElBruno.LocalEmbeddings.VectorData/Extensions/ServiceCollectionExtensions.cs`
  - Added `AddLocalEmbeddingsWithVectorStore(...)` overloads that register:
    - `IEmbeddingGenerator<string, Embedding<float>>` through existing core registration
    - `VectorStore` through a provider factory
  - Added `AddVectorStoreCollection<TKey, TRecord>(...)` to register typed `VectorStoreCollection<TKey, TRecord>` from DI.
- [x] **New tests project:** `tests/ElBruno.LocalEmbeddings.VectorData.Tests`
  - Added focused tests for DI registration and argument validation.
- [x] Added the new source/tests projects to `ElBruno.LocalEmbeddings.slnx`.
- [x] Updated docs (`README.md`, `docs/dependency-injection.md`, `docs/getting-started.md`, `docs/api-reference.md`) with VectorData guidance.
- [x] Added dedicated integration doc: `docs/vector-data-integration.md`.

---

### 5. Embedding Caching Middleware

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium

**Problem:** Repeated calls to `GenerateAsync` with the same input text re-run ONNX inference. In scenarios like RAG pipelines, the same documents are often re-embedded.

**Proposal:** Provide a caching `IEmbeddingGenerator` wrapper using the `Microsoft.Extensions.AI` middleware pattern.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Middleware/CachingEmbeddingGenerator.cs`
  - Wraps an `IEmbeddingGenerator<string, Embedding<float>>`
  - Uses `IDistributedCache` or `IMemoryCache` as the backing store
  - Cache key = SHA256 hash of the input text + model name
  - Implements `IEmbeddingGenerator<string, Embedding<float>>` (delegating handler pattern)

- **New file:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingGeneratorBuilderExtensions.cs`

  ```csharp
  public static IServiceCollection AddLocalEmbeddingsWithCaching(
      this IServiceCollection services,
      Action<LocalEmbeddingsOptions>? configure = null)
  {
      services.AddLocalEmbeddings(configure);
      services.AddMemoryCache();
      services.Decorate<IEmbeddingGenerator<string, Embedding<float>>, CachingEmbeddingGenerator>();
      return services;
  }
  ```

- **Alternative:** If `Microsoft.Extensions.AI` already provides `.UseDistributedCache()` for embeddings (like it does for `IChatClient`), then simply document how to use that instead of building a custom one. Check `Microsoft.Extensions.AI` v10.3.0 for embedding caching support.

---

### 6. ✅ True Async Model Loading

**Priority:** Medium | **Effort:** Low | **Impact:** Medium | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_0351.md](plan_260214_0351.md)

**Problem:** The constructor calls `.GetAwaiter().GetResult()` for model download, which blocks the calling thread. The `CreateAsync` factory wraps this in `Task.Run`, which consumes a thread-pool thread.

**Proposal:** Make model download truly async by restructuring initialization.

**Implementation details (completed):**

- [x] **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Added a shared constructor path that accepts a pre-resolved `modelDirectory`.
  - Moved async model directory resolution into `CreateAsync` (no `Task.Run` wrapper).
  - Kept synchronous constructor behavior for backwards compatibility.
  - Added quantized-aware model file resolution on load.

- [x] **File:** `tests/ElBruno.LocalEmbeddings.Tests/LocalEmbeddingGeneratorTests.cs`
  - Added `CreateAsync_WithNoModelPathAndEnsureDownloadedFalse_ThrowsInvalidOperationException` to validate async factory failure behavior.

- [x] **File:** `src/ElBruno.LocalEmbeddings/Extensions/ServiceCollectionExtensions.cs`
  - Preserved synchronous DI activation behavior while ensuring options-copy overload includes all relevant runtime options.

---

### 7. ✅ Quantized Model Support (INT8 / ONNX QDQ)

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_0351.md](plan_260214_0351.md)

**Problem:** Full-precision FP32 ONNX models use ~90 MB for all-MiniLM-L6-v2. INT8 quantized variants use ~23 MB with minimal quality loss.

**Proposal:** Support downloading and loading quantized model variants.

**Implementation details (completed):**

- [x] **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`
  - Added `PreferQuantized` option (default `false`).

- [x] **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - Added quantized preference parameter to `EnsureModelAsync`.
  - Implemented fallback order: `model_quantized.onnx` → `model_int8.onnx` → `model.onnx`.
  - Preserved existing tokenizer download behavior.

- [x] **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Added quantized-aware ONNX file selection when `PreferQuantized` is enabled.

- [x] **File:** `tests/ElBruno.LocalEmbeddings.Tests/ModelDownloaderTests.cs`
  - Added tests for quantized success path and FP32 fallback path.

- [x] **Documentation:**
  - Updated `docs/configuration.md` with quantized configuration and trade-offs.
  - Updated `docs/api-reference.md` options list.

---

### 8. Model Integrity Verification (SHA256)

**Priority:** Lower | **Effort:** Low | **Impact:** Low

**Problem:** Downloaded model files are not verified for integrity. Corrupted downloads or MITM attacks could lead to loading tampered models.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - After downloading a file, compute SHA256 and compare against expected hash.
  - HuggingFace Hub API provides file hashes at:

    ```
    GET https://huggingface.co/api/models/{modelName}/tree/main
    ```

    Response includes `{ "oid": "<sha256>", "size": <bytes> }` for each file.
  - Add optional `VerifyIntegrity` property to `LocalEmbeddingsOptions` (default `false` to avoid breaking changes, or `true` for secure-by-default).
  - On hash mismatch, delete the corrupted file and throw `InvalidOperationException`.

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`

  ```csharp
  public bool VerifyModelIntegrity { get; set; } = false;
  ```

---

### 15. ✅ Single-String Convenience API (DX)

**Priority:** High | **Effort:** Low | **Impact:** High | **Status:** ✅ Completed (2026-02-13)  
**Plan:** [plan_260213_0913.md](plan_260213_0913.md)

**Problem:** Every caller must wrap a single string in a collection (`["text"]`) and index the result with `[0]`, e.g. `await generator.GenerateAsync(["Hello"]); result[0].Vector`. This boilerplate hurts discoverability and first-run experience.

**Proposal:** Add two extension methods on `IEmbeddingGenerator<string, Embedding<float>>` in the `ElBruno.LocalEmbeddings` namespace:

| Method | Returns | Description |
|--------|---------|-------------|
| `GenerateAsync(string)` | `Task<GeneratedEmbeddings<Embedding<float>>>` | Wraps the single string in a collection and delegates |
| `GenerateEmbeddingAsync(string)` | `Task<Embedding<float>>` | Returns a single `Embedding<float>` directly — no indexing needed |

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/EmbeddingGeneratorExtensions.cs`
  - `public static class EmbeddingGeneratorExtensions` in `ElBruno.LocalEmbeddings` namespace
  - Both methods validate arguments with `ArgumentNullException.ThrowIfNull`
  - Full XML doc comments with `<example>` blocks

- **Tests:** 7 new tests in `tests/ElBruno.LocalEmbeddings.Tests/LocalEmbeddingGeneratorTests.cs`
  - Unit tests for null-guard on generator and value
  - Integration tests (SkippableFact) for correct dimensions, result equality with batch API

- **Samples:** Update all samples (`ConsoleApp`, `HelloWorldAltModel`, `RagChat`, etc.) to use the simpler API for single-text calls

- **Docs:** Update `README.md` Quick Start, `docs/api-reference.md`, `docs/getting-started.md` to showcase the convenience API

- **Why extension methods on the interface?** Works through DI where users receive `IEmbeddingGenerator<string, Embedding<float>>`, not the concrete `LocalEmbeddingGenerator`.

- **Why main namespace?** Methods appear in IntelliSense without extra `using` statements — best DX.

**Before:**

```csharp
var result = await generator.GenerateAsync(["Hello, world!"]);
float[] embedding = result[0].Vector.ToArray();
```

**After:**

```csharp
var embedding = await generator.GenerateEmbeddingAsync("Hello, world!");
float[] vector = embedding.Vector.ToArray();
```

---

### 16. Cross-Platform Native Runtime Reliability (Linux ARM64)

**Priority:** High | **Effort:** Low-Medium | **Impact:** High  
**Status:** Planned  
**Plan:** [plan_260213_1130.md](plan_260213_1130.md)

**Problem:** `samples/ConsoleApp` can fail on Linux ARM64 with ONNX Runtime native library load errors (`DllNotFoundException`) while Windows scenarios continue to work.

**Proposal:** Add a minimal Linux-only compatibility shim around ONNX Runtime native loading to preserve Windows behavior and improve cross-platform reliability.

**Implementation details:**

- Add Linux-only native load compatibility handling in `OnnxEmbeddingModel` initialization path.
- Keep Windows/macOS path unchanged and avoid API changes.
- Improve diagnostics when native loading fails (OS/arch + expected native artifacts).
- Validate through build/tests plus sample execution on Linux ARM64.

**Fallback strategy:**

- If the fix grows too platform-specific for core code, keep the canonical `ConsoleApp` and add a Linux-focused sample variant with explicit runtime guidance.

---

### 18. ✅ Dev Container Support (VS Code)

**Priority:** High | **Effort:** Low | **Impact:** High | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260213_1946.md](plan_260213_1946.md)

**Problem:** Contributors need local SDK/runtime setup, and environment drift can cause onboarding friction and inconsistent build/test behavior across machines.

**Proposal:** Add a repository-level VS Code Dev Container using .NET 10, with persistent NuGet and model caches and explicit contributor documentation.

**Implementation details:**

- **New file:** `.devcontainer/devcontainer.json`
  - Base image aligned with CI SDK (`10.0.x` lineage).
  - Workspace at repository root.
  - `postCreateCommand` to run `dotnet restore`.
  - Recommended extensions for C#/.NET developer workflow.
  - Named mounts/volumes for:
    - NuGet global package cache.
    - Local model cache used by runtime model downloads.

- **Docs updates:**
  - **File:** `docs/contributing.md`
    - Add Dev Container usage section (prerequisites, reopen flow, first-run model download note, validation sequence).

- **Validation:**
  - Run `dotnet restore`, `dotnet build`, and `dotnet test` from repository root in-container.

**Expected benefits:**

- Faster onboarding with zero local SDK setup for contributors.
- Better parity with CI/Linux behavior.
- Reduced repeated download costs via persistent caches.

---

## New Samples

### 17. ✅ RaspberryPiTiny — Ultra-Small Stability Sample

**Priority:** High | **Effort:** Low | **Impact:** High | **Status:** ✅ Completed (2026-02-13)

**Purpose:** Provide a minimal sample designed for low-resource devices (for example Raspberry Pi) where the full multi-scenario console sample can trigger device instability.

**Location:** `samples/RaspberryPiTiny/`

**What it does:**

- Default mode runs one embedding generation and exits quickly
- Optional `sim` mode runs two short embeddings and a cosine similarity check
- Uses conservative ONNX runtime settings (`ORT_SEQUENTIAL`, single thread)

**Key files:**

- `samples/RaspberryPiTiny/RaspberryPiTiny.csproj`
- `samples/RaspberryPiTiny/Program.cs`

---

### 9. VectorDataSample — Microsoft.Extensions.VectorData

**Priority:** High | **Effort:** Medium | **Impact:** High

**Purpose:** Show how `ElBruno.LocalEmbeddings` works with the official `Microsoft.Extensions.VectorData` abstractions, the pattern Microsoft promotes for vector search in .NET.

**Location:** `samples/VectorDataSample/`

**Key files:**

- `VectorDataSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.Extensions.VectorData.Abstractions" Version="9.7.0" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `CloudService.cs` — data model decorated with VectorData attributes:

  ```csharp
  internal class CloudService
  {
      [VectorStoreKey]
      public int Key { get; set; }

      [VectorStoreData]
      public string Name { get; set; }

      [VectorStoreData]
      public string Description { get; set; }

      [VectorStoreVector(Dimensions: 384, DistanceFunction = DistanceFunction.CosineSimilarity)]
      public ReadOnlyMemory<float> Vector { get; set; }
  }
  ```

- `Program.cs` — flow:
  1. Create `LocalEmbeddingGenerator` as `IEmbeddingGenerator<string, Embedding<float>>`
  2. Create `InMemoryVectorStore` from Semantic Kernel connectors
  3. Get a `VectorStoreCollection<int, CloudService>`
  4. Embed sample data and upsert into the collection
  5. Accept user queries, embed them, perform `SearchAsync`, display results

**What this demonstrates:**

- Drop-in compatibility with the standard .NET vector store ecosystem
- Users can swap `InMemory` for Qdrant, Azure AI Search, Cosmos DB, pgvector, etc. by changing one line

---

### 10. SemanticKernelSample — SK Plugin with Local Embeddings

**Priority:** Medium | **Effort:** Low | **Impact:** High

**Purpose:** Demonstrate Semantic Kernel with local embeddings for a fully offline AI assistant (Ollama for chat + local embeddings for memory).

**Location:** `samples/SemanticKernelSample/`

**Key files:**

- `SemanticKernelSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.SemanticKernel" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.Ollama" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs` — flow:
  1. Build a Semantic Kernel with Ollama chat completion
  2. Create `LocalEmbeddingGenerator` and register as the embedding generator with SK
  3. Create an in-memory vector store for semantic memory
  4. Import sample documents into memory
  5. Interactive chat loop where the kernel retrieves relevant memory before answering
  6. All running 100% locally — no API keys needed

**What this demonstrates:**

- Full Semantic Kernel integration
- Offline-first RAG pattern
- How `IEmbeddingGenerator` from M.E.AI plugs into SK's memory system

---

### 11. AspNetApiSample — Embedding Microservice

**Priority:** High | **Effort:** Low | **Impact:** High

**Purpose:** A minimal ASP.NET Core API exposing embedding generation as an HTTP endpoint, showcasing DI, health checks, and production patterns.

**Location:** `samples/AspNetApiSample/`

**Key files:**

- `AspNetApiSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.AspNetCore.OpenApi" Version="10.0.0" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs`:

  ```csharp
  var builder = WebApplication.CreateBuilder(args);

  builder.Services.AddLocalEmbeddings(options =>
  {
      options.ModelName = "sentence-transformers/all-MiniLM-L6-v2";
      options.EnsureModelDownloaded = true;
  });

  builder.Services.AddOpenApi();

  var app = builder.Build();

  app.MapOpenApi();

  app.MapPost("/api/embeddings", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      EmbedRequest request) =>
  {
      var result = await generator.GenerateAsync(request.Texts);
      return Results.Ok(new EmbedResponse
      {
          Embeddings = result.Select(e => e.Vector.ToArray()).ToList(),
          Model = generator.Metadata.DefaultModelId,
          Dimensions = generator.Metadata.DefaultModelDimensions
      });
  });

  app.MapPost("/api/similarity", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      SimilarityRequest request) =>
  {
      var embeddings = await generator.GenerateAsync([request.Text1, request.Text2]);
      var similarity = embeddings[0].CosineSimilarity(embeddings[1]);
      return Results.Ok(new { Similarity = similarity });
  });

  app.MapGet("/health", () => Results.Ok("healthy"));

  app.Run();
  ```

- `Models.cs` — request/response DTOs:

  ```csharp
  record EmbedRequest(string[] Texts);
  record EmbedResponse { List<float[]> Embeddings; string Model; int? Dimensions; }
  record SimilarityRequest(string Text1, string Text2);
  ```

**What this demonstrates:**

- Production DI registration pattern
- REST API compatible with OpenAI embeddings format
- Health endpoint for container orchestration
- OpenAPI/Swagger documentation

---

### 12. ✅ BenchmarkSample — BenchmarkDotNet Performance Suite

**Priority:** Medium | **Effort:** Low | **Impact:** Medium | **Status:** ✅ Completed (2026-02-14)  
**Plan:** [plan_260214_0759.md](plan_260214_0759.md)

**Purpose:** Provide reproducible performance benchmarks for different configurations, useful for documentation and regression testing.

**Location:** `samples/BenchmarkSample/`

**Implementation details (completed):**

- [x] **File:** `samples/BenchmarkSample/BenchmarkSample.csproj` — Console project referencing BenchmarkDotNet 0.14.0 + main library.
- [x] **File:** `samples/BenchmarkSample/Program.cs` — `BenchmarkSwitcher` entry point.
- [x] **File:** `samples/BenchmarkSample/EmbeddingBenchmarks.cs` — Single-text and batch (10/50/100) embedding throughput with `[MemoryDiagnoser]`.
- [x] **File:** `samples/BenchmarkSample/SimilarityBenchmarks.cs` — Cosine similarity (384/768-dim) and `FindClosest` (100/1000 corpus) benchmarks.
- [x] **File:** `samples/BenchmarkSample/TokenizerBenchmarks.cs` — Short text, long text, and batch (10/50) tokenization benchmarks.
- [x] **File:** `samples/BenchmarkSample/README.md` — Run instructions and baseline strategy.
- [x] **File:** `samples/BenchmarkSample/baselines/README.md` — Cross-platform baseline capture/comparison workflow.
- [x] Added to `ElBruno.LocalEmbeddings.slnx`.
- [x] Updated `samples/README.md` with BenchmarkSample entry.

**What this demonstrates:**

- Throughput for single vs. batch inference
- Memory allocation profiles
- Cross-platform baseline strategy for Windows / Linux regression detection
- TensorPrimitives cosine similarity and nearest-neighbour search performance

---

### 13. ImageSearchSample — Multimodal CLIP Search

**Priority:** Lower | **Effort:** High | **Impact:** Medium

**Purpose:** Demonstrate text-to-image semantic search using CLIP-style models, all running locally.

**Location:** `samples/ImageSearchSample/`

**Prerequisites:**

- A CLIP model exported to ONNX (e.g., `openai/clip-vit-base-patch32` from HuggingFace, ONNX export)
- CLIP uses separate text and image encoders; the library would need to support loading two ONNX models or the sample handles image encoding separately

**Flow:**

1. Load a folder of images
2. Generate image embeddings using the CLIP vision encoder ONNX model
3. Accept natural language queries
4. Generate text embeddings using the CLIP text encoder
5. Compute cosine similarity between text embedding and all image embeddings
6. Display top-K matching images

**Note:** This sample may require library changes to support non-BERT tokenizers (CLIP uses a BPE tokenizer, not WordPiece). This could motivate a tokenizer abstraction or a separate `ClipEmbeddingGenerator` class. Consider this a stretch goal.

---

### 14. MAUISample — Desktop / Mobile Semantic Note Search

**Priority:** Lower | **Effort:** Medium | **Impact:** Medium

**Purpose:** Show the library working in a .NET MAUI app (Windows, macOS, potentially Android/iOS), proving it's not server-only.

**Location:** `samples/MAUISample/`

**Flow:**

1. User types notes into a text editor
2. Notes are embedded and stored locally (SQLite + embeddings)
3. User can semantically search their notes ("find my notes about cooking recipes")
4. Results ranked by cosine similarity

**Implementation considerations:**

- ONNX Runtime supports Android (via NNAPI) and iOS (via CoreML), but the NuGet packages differ
- The model download should happen on first launch with a progress indicator
- Use `IEmbeddingGenerator<string, Embedding<float>>` via DI in MAUI's `MauiProgram.cs`

---

## Priority Matrix

| Priority | Item | Effort | Impact | Dependencies | Completed |
|----------|------|--------|--------|--------------|-----------|
| **High** | 11. AspNetApiSample — Embedding Microservice | Low | High | None | No |
| **Medium** | 12. ✅ BenchmarkSample — BenchmarkDotNet Performance Suite | Low | Medium | After #2 (to compare before/after SIMD) | Yes |
| **High** | 16. Cross-Platform Native Runtime Reliability (Linux ARM64) | Low-Medium | High | None | No |
| **Medium** | 5. Embedding Caching Middleware | Medium | Medium | Check M.E.AI built-in support first | No |
| **Medium** | 3. GPU / Hardware Acceleration (DirectML, CUDA) | Medium | High | None | No |
| **Lower** | 13. ImageSearchSample — Multimodal CLIP Search | High | Medium | Library tokenizer abstraction needed | No |
| **Lower** | 14. MAUISample — Desktop / Mobile Semantic Note Search | Medium | Medium | Multi-TFM (#1) helpful | No |
| **Lower** | 8. Model Integrity Verification (SHA256) | Low | Low | None | No |
| **Medium** | 7. ✅ Quantized Model Support (INT8 / ONNX QDQ) | Medium | Medium | None | Yes |
| **Medium** | 10. SemanticKernelSample — SK Plugin with Local Embeddings | Low | High | None | No |
| **Medium** | 6. ✅ True Async Model Loading | Low | Medium | None | Yes |
| **High** | 9. VectorDataSample — Microsoft.Extensions.VectorData | Medium | High | None (uses existing API) | No |
| **High** | 18. ✅ Dev Container Support (VS Code) | Low | High | None | Yes |
| **Medium** | 4. ✅ Microsoft.Extensions.VectorData Integration | Medium | High | None | Yes |
| **High** | 1. ✅ Multi-TFM Support (net8.0 + net10.0) | Low | Very High | None | Yes |
| **High** | 17. ✅ RaspberryPiTiny — Ultra-Small Stability Sample | Low | High | #16 helpful | Yes |
| **High** | 2. ✅ SIMD / TensorPrimitives for Similarity & Normalization | Low | Medium | None (or after #1) | Yes |
| **High** | 19. ✅ Issue #24 Reliability, DX & Test Coverage Sweep | Medium | High | None | Yes |
| **High** | 15. ✅ Single-String Convenience API (DX) | Low | High | None | Yes |

### Suggested implementation order

1. **Next features/samples (alphabetical):**
   - #11 AspNetApiSample — Embedding Microservice
   - #16 Cross-Platform Native Runtime Reliability (Linux ARM64)
   - #5 Embedding Caching Middleware
   - #3 GPU / Hardware Acceleration (DirectML, CUDA)
   - #13 ImageSearchSample — Multimodal CLIP Search
   - #14 MAUISample — Desktop / Mobile Semantic Note Search
   - #8 Model Integrity Verification (SHA256)
   - #10 SemanticKernelSample — SK Plugin with Local Embeddings
   - #9 VectorDataSample — Microsoft.Extensions.VectorData

2. **Completed (moved to bottom, alphabetical):**
   - #12 ✅ BenchmarkSample — BenchmarkDotNet Performance Suite
   - #18 ✅ Dev Container Support (VS Code)
   - #4 ✅ Microsoft.Extensions.VectorData Integration
   - #1 ✅ Multi-TFM Support (net8.0 + net10.0)
   - #17 ✅ RaspberryPiTiny — Ultra-Small Stability Sample
   - #2 ✅ SIMD / TensorPrimitives for Similarity & Normalization
   - #15 ✅ Single-String Convenience API (DX)
   - #7 ✅ Quantized Model Support (INT8 / ONNX QDQ)
   - #6 ✅ True Async Model Loading
