# ElBruno.LocalEmbeddings — Roadmap & Feature Proposals

**Created:** 2026-02-13 08:03  
**Status:** Draft — ready for implementation planning

---

## Table of Contents

1. [Library Improvements](#library-improvements)
   - [1. Multi-TFM Support (net8.0 + net10.0)](#1-multi-tfm-support-net80--net100)
   - [2. SIMD / TensorPrimitives for Similarity & Normalization](#2-simd--tensorprimitives-for-similarity--normalization)
   - [3. GPU / Hardware Acceleration (DirectML, CUDA)](#3-gpu--hardware-acceleration-directml-cuda)
   - [4. Microsoft.Extensions.VectorData Integration](#4-microsoftextensionsvectordata-integration)
   - [5. Embedding Caching Middleware](#5-embedding-caching-middleware)
   - [6. True Async Model Loading](#6-true-async-model-loading)
   - [7. Quantized Model Support (INT8 / ONNX QDQ)](#7-quantized-model-support-int8--onnx-qdq)
   - [8. Model Integrity Verification (SHA256)](#8-model-integrity-verification-sha256)
2. [New Samples](#new-samples)
   - [9. VectorDataSample — Microsoft.Extensions.VectorData](#9-vectordatasample--microsoftextensionsvectordata)
   - [10. SemanticKernelSample — SK Plugin with Local Embeddings](#10-semantickernelsample--sk-plugin-with-local-embeddings)
   - [11. AspNetApiSample — Embedding Microservice](#11-aspnetapisample--embedding-microservice)
   - [12. BenchmarkSample — BenchmarkDotNet Performance Suite](#12-benchmarksample--benchmarkdotnet-performance-suite)
   - [13. ImageSearchSample — Multimodal CLIP Search](#13-imagesearchsample--multimodal-clip-search)
   - [14. MAUISample — Desktop / Mobile Semantic Note Search](#14-mauisample--desktop--mobile-semantic-note-search)
3. [Priority Matrix](#priority-matrix)

---

## Library Improvements

### 1. Multi-TFM Support (net8.0 + net10.0)

**Priority:** High | **Effort:** Low | **Impact:** Very High

**Problem:** The library targets only `net10.0`. .NET 8 is the current LTS release and the most widely deployed runtime.

**Proposal:** Add `net8.0` as an additional target framework in both `ElBruno.LocalEmbeddings` and `ElBruno.LocalEmbeddings.KernelMemory`.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj`
  - Change `<TargetFramework>net10.0</TargetFramework>` → `<TargetFrameworks>net8.0;net10.0</TargetFrameworks>`
- **File:** `src/ElBruno.LocalEmbeddings.KernelMemory/ElBruno.LocalEmbeddings.KernelMemory.csproj`
  - Same TFM change.
- **File:** `tests/ElBruno.LocalEmbeddings.Tests/ElBruno.LocalEmbeddings.Tests.csproj`
  - Target both TFMs so tests run on both.
- **File:** `tests/ElBruno.LocalEmbeddings.KernelMemory.Tests/ElBruno.LocalEmbeddings.KernelMemory.Tests.csproj`
  - Same.
- Review all `PackageReference` versions for net8.0 compatibility:
  - `Microsoft.Extensions.AI.Abstractions` — check if `10.3.0` supports net8.0 or if a conditional version is needed.
  - `Microsoft.Extensions.DependencyInjection.Abstractions`, `Microsoft.Extensions.Http`, `Microsoft.Extensions.Options`, `Microsoft.Extensions.Options.ConfigurationExtensions` — verify `10.0.3` versions support net8.0 or pin conditionally.
  - `Microsoft.ML.OnnxRuntime 1.24.1` — supports net8.0 ✓
  - `Microsoft.ML.Tokenizers 2.0.0` — supports net8.0 ✓
- Use `#if NET10_0_OR_GREATER` preprocessor directives if any API differs between TFMs.
- Update `Directory.Build.props` if shared TFM settings are needed.
- Ensure all samples still target `net10.0` (samples don't need multi-TFM).

---

### 2. SIMD / TensorPrimitives for Similarity & Normalization

**Priority:** High | **Effort:** Low | **Impact:** Medium

**Problem:** `EmbeddingExtensions.CosineSimilarity` and `OnnxEmbeddingModel.L2Normalize` use scalar `for` loops. For high-dimensional vectors (384+ floats), SIMD provides 4-8x speedups.

**Proposal:** Replace scalar math with `System.Numerics.Tensors.TensorPrimitives` (available since .NET 8).

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingExtensions.cs`
  - Replace `CosineSimilarity(ReadOnlyMemory<float>, ReadOnlyMemory<float>)` body:

    ```csharp
    // Before (scalar loop):
    float dotProduct = 0; float normA = 0; float normB = 0;
    for (int i = 0; i < spanA.Length; i++) { ... }

    // After:
    return TensorPrimitives.CosineSimilarity(spanA, spanB);
    ```

  - Add `using System.Numerics.Tensors;`

- **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs`
  - Replace `L2Normalize(float[])` body:

    ```csharp
    // Before:
    var norm = MathF.Sqrt(vector.Sum(x => x * x));
    if (norm > 0) { for (int i ...) vector[i] /= norm; }

    // After:
    var norm = TensorPrimitives.Norm(vector);
    if (norm > 0)
    {
        TensorPrimitives.Divide(vector, norm, vector);
    }
    ```

  - Add `using System.Numerics.Tensors;`

- **NuGet dependency:** Add `<PackageReference Include="System.Numerics.Tensors" Version="9.0.3" />` (or latest). On .NET 8+, `TensorPrimitives` is inbox or available as a package.

- If multi-TFM is implemented first, `TensorPrimitives` is available on both net8.0 and net10.0. No conditional compilation needed.

- **Tests:** Existing `LocalEmbeddingGeneratorTests` should cover this since the behavior is unchanged; add a dedicated cosine similarity unit test comparing known vectors if not already present.

---

### 3. GPU / Hardware Acceleration (DirectML, CUDA)

**Priority:** Medium | **Effort:** Medium | **Impact:** High

**Problem:** The library uses `Microsoft.ML.OnnxRuntime` (CPU-only). Users with GPUs get no acceleration benefit.

**Proposal:** Add an `ExecutionProvider` enum option and companion NuGet packages for GPU execution providers.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Options/ExecutionProviderType.cs`

  ```csharp
  namespace ElBruno.LocalEmbeddings.Options;

  public enum ExecutionProviderType
  {
      Cpu,
      DirectML,    // Windows GPU via DirectX
      Cuda,        // NVIDIA GPU
      CoreML       // macOS Apple Silicon
  }
  ```

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`
  - Add property:

    ```csharp
    public ExecutionProviderType ExecutionProvider { get; set; } = ExecutionProviderType.Cpu;
    ```

  - Add optional GPU device ID:

    ```csharp
    public int DeviceId { get; set; } = 0;
    ```

- **File:** `src/ElBruno.LocalEmbeddings/OnnxEmbeddingModel.cs` — `Load()` method
  - After creating `SessionOptions`, append execution provider before creating `InferenceSession`:

    ```csharp
    switch (executionProvider)
    {
        case ExecutionProviderType.DirectML:
            sessionOptions.AppendExecutionProvider_DML(deviceId);
            break;
        case ExecutionProviderType.Cuda:
            sessionOptions.AppendExecutionProvider_CUDA(deviceId);
            break;
        case ExecutionProviderType.CoreML:
            sessionOptions.AppendExecutionProvider_CoreML();
            break;
    }
    ```

  - The `Load()` method signature needs to accept `ExecutionProviderType` and `int deviceId` parameters (or receive the full options object).

- **Companion NuGet packages (optional, future):**
  - `ElBruno.LocalEmbeddings.DirectML` — depends on `Microsoft.ML.OnnxRuntime.DirectML`
  - `ElBruno.LocalEmbeddings.Cuda` — depends on `Microsoft.ML.OnnxRuntime.Gpu`
  - These would be thin metapackages that pull in the right native binaries.

- **Alternative simpler approach:** Document that users can swap out the `Microsoft.ML.OnnxRuntime` NuGet for `Microsoft.ML.OnnxRuntime.DirectML` or `Microsoft.ML.OnnxRuntime.Gpu` in their project, and the same `SessionOptions` API works. The library just needs to expose the `ExecutionProvider` option to configure the session.

- **Testing:** GPU tests should use `[SkippableFact]` and check for GPU availability at runtime.

---

### 4. Microsoft.Extensions.VectorData Integration

**Priority:** High (ecosystem alignment) | **Effort:** Medium | **Impact:** High

**Problem:** The library includes a custom `InMemoryVectorStore` in the RagChat sample and custom `FindClosest` extensions. The .NET ecosystem is converging on `Microsoft.Extensions.VectorData.Abstractions` (v9.7.0) as the standard for vector stores.

**Proposal:** Create a companion package `ElBruno.LocalEmbeddings.VectorData` that provides integration with the `Microsoft.Extensions.VectorData` ecosystem.

**Implementation details:**

- **New project:** `src/ElBruno.LocalEmbeddings.VectorData/ElBruno.LocalEmbeddings.VectorData.csproj`

  ```xml
  <PackageReference Include="Microsoft.Extensions.VectorData.Abstractions" Version="9.7.0" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  ```

- **New file:** `src/ElBruno.LocalEmbeddings.VectorData/Extensions/ServiceCollectionExtensions.cs`
  - Extension method `AddLocalEmbeddingsWithVectorStore()` that registers:
    - `IEmbeddingGenerator<string, Embedding<float>>` (the existing local generator)
    - `VectorStore` (from `Microsoft.Extensions.VectorData`)
  - Example usage:

    ```csharp
    services.AddLocalEmbeddings(o => { o.ModelName = "..."; })
            .AddInMemoryVectorStore();  // or .AddSqliteVecStore(), etc.
    ```

- **Key types from M.E.VectorData to integrate with:**
  - `VectorStoreCollection<TKey, TRecord>` — for CRUD + search
  - `VectorStoreDataAttribute`, `VectorStoreKeyAttribute`, `VectorStoreVectorAttribute` — record decorators
  - `IVectorSearchable<TRecord>` — search interface
  - `VectorSearchOptions<TRecord>` and `VectorSearchResult<TRecord>` — search options/results

- **Migration path:** Update the `RagChat` sample to optionally use `VectorStoreCollection` instead of the custom `InMemoryVectorStore`.

---

### 5. Embedding Caching Middleware

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium

**Problem:** Repeated calls to `GenerateAsync` with the same input text re-run ONNX inference. In scenarios like RAG pipelines, the same documents are often re-embedded.

**Proposal:** Provide a caching `IEmbeddingGenerator` wrapper using the `Microsoft.Extensions.AI` middleware pattern.

**Implementation details:**

- **New file:** `src/ElBruno.LocalEmbeddings/Middleware/CachingEmbeddingGenerator.cs`
  - Wraps an `IEmbeddingGenerator<string, Embedding<float>>`
  - Uses `IDistributedCache` or `IMemoryCache` as the backing store
  - Cache key = SHA256 hash of the input text + model name
  - Implements `IEmbeddingGenerator<string, Embedding<float>>` (delegating handler pattern)

- **New file:** `src/ElBruno.LocalEmbeddings/Extensions/EmbeddingGeneratorBuilderExtensions.cs`

  ```csharp
  public static IServiceCollection AddLocalEmbeddingsWithCaching(
      this IServiceCollection services,
      Action<LocalEmbeddingsOptions>? configure = null)
  {
      services.AddLocalEmbeddings(configure);
      services.AddMemoryCache();
      services.Decorate<IEmbeddingGenerator<string, Embedding<float>>, CachingEmbeddingGenerator>();
      return services;
  }
  ```

- **Alternative:** If `Microsoft.Extensions.AI` already provides `.UseDistributedCache()` for embeddings (like it does for `IChatClient`), then simply document how to use that instead of building a custom one. Check `Microsoft.Extensions.AI` v10.3.0 for embedding caching support.

---

### 6. True Async Model Loading

**Priority:** Medium | **Effort:** Low | **Impact:** Medium

**Problem:** The constructor calls `.GetAwaiter().GetResult()` for model download, which blocks the calling thread. The `CreateAsync` factory wraps this in `Task.Run`, which consumes a thread-pool thread.

**Proposal:** Make model download truly async by restructuring initialization.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - Make the constructor accept a pre-resolved `modelDirectory` (no download logic).
  - Move download logic entirely into `CreateAsync`:

    ```csharp
    public static async Task<LocalEmbeddingGenerator> CreateAsync(
        LocalEmbeddingsOptions options,
        CancellationToken cancellationToken = default)
    {
        string modelDirectory;
        if (!string.IsNullOrWhiteSpace(options.ModelPath))
        {
            modelDirectory = options.ModelPath;
        }
        else if (options.EnsureModelDownloaded)
        {
            var downloader = new ModelDownloader(new HttpClient(), options.CacheDirectory);
            modelDirectory = await downloader.EnsureModelAsync(options.ModelName, cancellationToken: cancellationToken);
        }
        else { throw ... }

        return new LocalEmbeddingGenerator(modelDirectory, options);
    }
    ```

  - Add a private/internal constructor that takes `modelDirectory` + options.
  - Keep the existing public constructor for backwards compatibility (it can still do sync download for simple console app scenarios).

- **File:** `src/ElBruno.LocalEmbeddings/Extensions/ServiceCollectionExtensions.cs`
  - The DI registration factory is synchronous. Consider using `IHostedService` or lazy initialization pattern:

    ```csharp
    services.TryAddSingleton<IEmbeddingGenerator<string, Embedding<float>>>(sp =>
    {
        var options = sp.GetRequiredService<IOptions<LocalEmbeddingsOptions>>().Value;
        // Still sync here (DI factory), but at least the CreateAsync path exists for manual use
        return new LocalEmbeddingGenerator(options);
    });
    ```

  - Alternatively, register as `Lazy<Task<LocalEmbeddingGenerator>>` or use an `IHostedService` that pre-warms the model on startup.

---

### 7. Quantized Model Support (INT8 / ONNX QDQ)

**Priority:** Medium | **Effort:** Medium | **Impact:** Medium

**Problem:** Full-precision FP32 ONNX models use ~90 MB for all-MiniLM-L6-v2. INT8 quantized variants use ~23 MB with minimal quality loss.

**Proposal:** Support downloading and loading quantized model variants.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`

  ```csharp
  /// <summary>
  /// Gets or sets whether to prefer a quantized (INT8) model variant if available.
  /// Default is false (uses full-precision FP32 model).
  /// </summary>
  public bool PreferQuantized { get; set; } = false;
  ```

- **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - In `EnsureModelAsync`, when `PreferQuantized` is true, try downloading from:

    ```
    https://huggingface.co/{modelName}/resolve/main/onnx/model_quantized.onnx
    ```

    If that 404s, fall back to the regular `model.onnx`.
  - The quantized model file is typically named `model_quantized.onnx` or `model_int8.onnx` on HuggingFace. Try both names.

- **File:** `src/ElBruno.LocalEmbeddings/LocalEmbeddingGenerator.cs`
  - When loading, check for `model_quantized.onnx` first if `PreferQuantized` is set, fall back to `model.onnx`.

- **Documentation:** Add a section in `docs/configuration.md` explaining the trade-offs (file size, memory, speed vs. quality).

---

### 8. Model Integrity Verification (SHA256)

**Priority:** Lower | **Effort:** Low | **Impact:** Low

**Problem:** Downloaded model files are not verified for integrity. Corrupted downloads or MITM attacks could lead to loading tampered models.

**Implementation details:**

- **File:** `src/ElBruno.LocalEmbeddings/ModelDownloader.cs`
  - After downloading a file, compute SHA256 and compare against expected hash.
  - HuggingFace Hub API provides file hashes at:

    ```
    GET https://huggingface.co/api/models/{modelName}/tree/main
    ```

    Response includes `{ "oid": "<sha256>", "size": <bytes> }` for each file.
  - Add optional `VerifyIntegrity` property to `LocalEmbeddingsOptions` (default `false` to avoid breaking changes, or `true` for secure-by-default).
  - On hash mismatch, delete the corrupted file and throw `InvalidOperationException`.

- **File:** `src/ElBruno.LocalEmbeddings/Options/LocalEmbeddingsOptions.cs`

  ```csharp
  public bool VerifyModelIntegrity { get; set; } = false;
  ```

---

## New Samples

### 9. VectorDataSample — Microsoft.Extensions.VectorData

**Priority:** High | **Effort:** Medium | **Impact:** High

**Purpose:** Show how `ElBruno.LocalEmbeddings` works with the official `Microsoft.Extensions.VectorData` abstractions, the pattern Microsoft promotes for vector search in .NET.

**Location:** `samples/VectorDataSample/`

**Key files:**

- `VectorDataSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.Extensions.VectorData.Abstractions" Version="9.7.0" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `CloudService.cs` — data model decorated with VectorData attributes:

  ```csharp
  internal class CloudService
  {
      [VectorStoreKey]
      public int Key { get; set; }

      [VectorStoreData]
      public string Name { get; set; }

      [VectorStoreData]
      public string Description { get; set; }

      [VectorStoreVector(Dimensions: 384, DistanceFunction = DistanceFunction.CosineSimilarity)]
      public ReadOnlyMemory<float> Vector { get; set; }
  }
  ```

- `Program.cs` — flow:
  1. Create `LocalEmbeddingGenerator` as `IEmbeddingGenerator<string, Embedding<float>>`
  2. Create `InMemoryVectorStore` from Semantic Kernel connectors
  3. Get a `VectorStoreCollection<int, CloudService>`
  4. Embed sample data and upsert into the collection
  5. Accept user queries, embed them, perform `SearchAsync`, display results

**What this demonstrates:**

- Drop-in compatibility with the standard .NET vector store ecosystem
- Users can swap `InMemory` for Qdrant, Azure AI Search, Cosmos DB, pgvector, etc. by changing one line

---

### 10. SemanticKernelSample — SK Plugin with Local Embeddings

**Priority:** Medium | **Effort:** Low | **Impact:** High

**Purpose:** Demonstrate Semantic Kernel with local embeddings for a fully offline AI assistant (Ollama for chat + local embeddings for memory).

**Location:** `samples/SemanticKernelSample/`

**Key files:**

- `SemanticKernelSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.SemanticKernel" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.InMemory" Version="1.x" />
  <PackageReference Include="Microsoft.SemanticKernel.Connectors.Ollama" Version="1.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs` — flow:
  1. Build a Semantic Kernel with Ollama chat completion
  2. Create `LocalEmbeddingGenerator` and register as the embedding generator with SK
  3. Create an in-memory vector store for semantic memory
  4. Import sample documents into memory
  5. Interactive chat loop where the kernel retrieves relevant memory before answering
  6. All running 100% locally — no API keys needed

**What this demonstrates:**

- Full Semantic Kernel integration
- Offline-first RAG pattern
- How `IEmbeddingGenerator` from M.E.AI plugs into SK's memory system

---

### 11. AspNetApiSample — Embedding Microservice

**Priority:** High | **Effort:** Low | **Impact:** High

**Purpose:** A minimal ASP.NET Core API exposing embedding generation as an HTTP endpoint, showcasing DI, health checks, and production patterns.

**Location:** `samples/AspNetApiSample/`

**Key files:**

- `AspNetApiSample.csproj` — references:

  ```xml
  <PackageReference Include="Microsoft.AspNetCore.OpenApi" Version="10.0.0" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `Program.cs`:

  ```csharp
  var builder = WebApplication.CreateBuilder(args);

  builder.Services.AddLocalEmbeddings(options =>
  {
      options.ModelName = "sentence-transformers/all-MiniLM-L6-v2";
      options.EnsureModelDownloaded = true;
  });

  builder.Services.AddOpenApi();

  var app = builder.Build();

  app.MapOpenApi();

  app.MapPost("/api/embeddings", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      EmbedRequest request) =>
  {
      var result = await generator.GenerateAsync(request.Texts);
      return Results.Ok(new EmbedResponse
      {
          Embeddings = result.Select(e => e.Vector.ToArray()).ToList(),
          Model = generator.Metadata.DefaultModelId,
          Dimensions = generator.Metadata.DefaultModelDimensions
      });
  });

  app.MapPost("/api/similarity", async (
      IEmbeddingGenerator<string, Embedding<float>> generator,
      SimilarityRequest request) =>
  {
      var embeddings = await generator.GenerateAsync([request.Text1, request.Text2]);
      var similarity = embeddings[0].CosineSimilarity(embeddings[1]);
      return Results.Ok(new { Similarity = similarity });
  });

  app.MapGet("/health", () => Results.Ok("healthy"));

  app.Run();
  ```

- `Models.cs` — request/response DTOs:

  ```csharp
  record EmbedRequest(string[] Texts);
  record EmbedResponse { List<float[]> Embeddings; string Model; int? Dimensions; }
  record SimilarityRequest(string Text1, string Text2);
  ```

**What this demonstrates:**

- Production DI registration pattern
- REST API compatible with OpenAI embeddings format
- Health endpoint for container orchestration
- OpenAPI/Swagger documentation

---

### 12. BenchmarkSample — BenchmarkDotNet Performance Suite

**Priority:** Medium | **Effort:** Low | **Impact:** Medium

**Purpose:** Provide reproducible performance benchmarks for different configurations, useful for documentation and regression testing.

**Location:** `samples/BenchmarkSample/`

**Key files:**

- `BenchmarkSample.csproj`:

  ```xml
  <PackageReference Include="BenchmarkDotNet" Version="0.14.x" />
  <ProjectReference Include="../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj" />
  ```

- `EmbeddingBenchmarks.cs`:

  ```csharp
  [MemoryDiagnoser]
  [SimpleJob(RuntimeMoniker.Net100)]
  public class EmbeddingBenchmarks
  {
      private LocalEmbeddingGenerator _generator;
      private string[] _singleText;
      private string[] _batchTexts;

      [GlobalSetup]
      public void Setup()
      {
          _generator = new LocalEmbeddingGenerator(new LocalEmbeddingsOptions
          {
              ModelName = "sentence-transformers/all-MiniLM-L6-v2"
          });
          _singleText = ["The quick brown fox jumps over the lazy dog."];
          _batchTexts = Enumerable.Range(0, 100)
              .Select(i => $"Sample sentence number {i} for benchmarking embeddings.")
              .ToArray();
      }

      [Benchmark(Baseline = true)]
      public Task<GeneratedEmbeddings<Embedding<float>>> SingleEmbedding()
          => _generator.GenerateAsync(_singleText);

      [Benchmark]
      [Arguments(10)]
      [Arguments(50)]
      [Arguments(100)]
      public Task<GeneratedEmbeddings<Embedding<float>>> BatchEmbedding(int count)
          => _generator.GenerateAsync(_batchTexts.Take(count));

      [GlobalCleanup]
      public void Cleanup() => _generator.Dispose();
  }
  ```

- `SimilarityBenchmarks.cs` — benchmarks cosine similarity (scalar vs. TensorPrimitives, before/after the SIMD improvement).

**What this demonstrates:**

- Throughput for single vs. batch inference
- Memory allocation profiles
- Baseline numbers for the README / docs

---

### 13. ImageSearchSample — Multimodal CLIP Search

**Priority:** Lower | **Effort:** High | **Impact:** Medium

**Purpose:** Demonstrate text-to-image semantic search using CLIP-style models, all running locally.

**Location:** `samples/ImageSearchSample/`

**Prerequisites:**

- A CLIP model exported to ONNX (e.g., `openai/clip-vit-base-patch32` from HuggingFace, ONNX export)
- CLIP uses separate text and image encoders; the library would need to support loading two ONNX models or the sample handles image encoding separately

**Flow:**

1. Load a folder of images
2. Generate image embeddings using the CLIP vision encoder ONNX model
3. Accept natural language queries
4. Generate text embeddings using the CLIP text encoder
5. Compute cosine similarity between text embedding and all image embeddings
6. Display top-K matching images

**Note:** This sample may require library changes to support non-BERT tokenizers (CLIP uses a BPE tokenizer, not WordPiece). This could motivate a tokenizer abstraction or a separate `ClipEmbeddingGenerator` class. Consider this a stretch goal.

---

### 14. MAUISample — Desktop / Mobile Semantic Note Search

**Priority:** Lower | **Effort:** Medium | **Impact:** Medium

**Purpose:** Show the library working in a .NET MAUI app (Windows, macOS, potentially Android/iOS), proving it's not server-only.

**Location:** `samples/MAUISample/`

**Flow:**

1. User types notes into a text editor
2. Notes are embedded and stored locally (SQLite + embeddings)
3. User can semantically search their notes ("find my notes about cooking recipes")
4. Results ranked by cosine similarity

**Implementation considerations:**

- ONNX Runtime supports Android (via NNAPI) and iOS (via CoreML), but the NuGet packages differ
- The model download should happen on first launch with a progress indicator
- Use `IEmbeddingGenerator<string, Embedding<float>>` via DI in MAUI's `MauiProgram.cs`

---

## Priority Matrix

| Priority | Item | Effort | Impact | Dependencies |
|----------|------|--------|--------|-------------|
| **High** | 1. Multi-TFM (net8.0 + net10.0) | Low | Very High | None |
| **High** | 2. SIMD/TensorPrimitives | Low | Medium | None (or after #1) |
| **High** | 9. VectorData sample | Medium | High | None (uses existing API) |
| **High** | 11. ASP.NET API sample | Low | High | None |
| **Medium** | 3. GPU acceleration | Medium | High | None |
| **Medium** | 4. VectorData companion package | Medium | High | After #9 (sample validates the design) |
| **Medium** | 5. Caching middleware | Medium | Medium | Check M.E.AI built-in support first |
| **Medium** | 6. Async model loading | Low | Medium | None |
| **Medium** | 7. Quantized model support | Medium | Medium | None |
| **Medium** | 10. Semantic Kernel sample | Low | High | None |
| **Medium** | 12. Benchmark sample | Low | Medium | After #2 (to compare before/after SIMD) |
| **Lower** | 8. Model integrity (SHA256) | Low | Low | None |
| **Lower** | 13. Image search (CLIP) | High | Medium | Library tokenizer abstraction needed |
| **Lower** | 14. MAUI sample | Medium | Medium | Multi-TFM (#1) helpful |

### Suggested implementation order

1. **Phase 1 — Quick wins:**
   - #1 Multi-TFM support
   - #2 SIMD/TensorPrimitives
   - #11 ASP.NET API sample
   - #9 VectorData sample

2. **Phase 2 — Ecosystem integration:**
   - #4 VectorData companion package
   - #10 Semantic Kernel sample
   - #6 Async model loading
   - #12 Benchmark sample

3. **Phase 3 — Advanced features:**
   - #3 GPU acceleration
   - #7 Quantized model support
   - #5 Caching middleware
   - #8 Model integrity verification

4. **Phase 4 — Stretch goals:**
   - #13 Image search (CLIP)
   - #14 MAUI sample
