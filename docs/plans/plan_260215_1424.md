# Plan — ImageSearchSample: Multimodal CLIP Search (#12)

**Created:** 2026-02-15 14:24  
**Scope:** Implement roadmap item #12 (`ImageSearchSample — Multimodal CLIP Search`), a sample application demonstrating text-to-image semantic search using CLIP-style models running entirely locally via ONNX Runtime.

## Goals

- Create a new console sample `samples/ImageSearchSample/` that demonstrates multimodal text-to-image semantic search.
- Use a CLIP model exported to ONNX (e.g., `openai/clip-vit-base-patch32`) for both text and image encoding.
- Accept natural language queries and return top-K matching images ranked by cosine similarity.
- Demonstrate the library's extensibility beyond text-only embedding models.

## Background

CLIP (Contrastive Language–Image Pretraining) models encode text and images into a shared embedding space, enabling cross-modal search. Unlike the library's current BERT-based text embedding models, CLIP requires:

1. **Two separate ONNX models** — a text encoder and a vision (image) encoder.
2. **A BPE tokenizer** for text (not WordPiece as used by BERT/MiniLM models).
3. **Image preprocessing** — resizing, center-cropping, normalization to match CLIP's expected input format.

This sample will handle image encoding directly (using ONNX Runtime for inference on the vision model) while demonstrating how the library's cosine similarity utilities can be reused for ranking results.

## Prerequisites

- **CLIP ONNX models:** The text encoder and vision encoder must be available as separate `.onnx` files. These can be exported from HuggingFace using `optimum` or downloaded from repos that provide pre-exported ONNX variants (e.g., `Xenova/clip-vit-base-patch32` on HuggingFace).
- **Sample images:** Include a small set of sample images (5–10) in a `samples/ImageSearchSample/images/` folder, or document how to provide a custom folder.
- **NuGet packages:**
  - `Microsoft.ML.OnnxRuntime` — already used by the library
  - `SixLabors.ImageSharp` — for cross-platform image loading, resizing, and pixel extraction
  - `System.Numerics.Tensors` — already used by the library for TensorPrimitives

## Architecture

```
samples/ImageSearchSample/
├── ImageSearchSample.csproj
├── Program.cs                  # Main entry point — orchestrates the flow
├── ClipTextEncoder.cs          # Loads CLIP text ONNX model, tokenizes + encodes text
├── ClipImageEncoder.cs         # Loads CLIP vision ONNX model, preprocesses + encodes images
├── ClipTokenizer.cs            # BPE tokenizer for CLIP text encoding
├── ImageSearchEngine.cs        # Builds index, performs cosine similarity search
├── README.md                   # Run instructions, model download steps
└── images/                     # Sample images (or .gitkeep + instructions)
    └── README.md               # Instructions for obtaining sample images
```

## Implementation Checklist

- [ ] **Scaffold the project**
  - Create `samples/ImageSearchSample/ImageSearchSample.csproj` targeting `net10.0`.
  - Add package references: `Microsoft.ML.OnnxRuntime`, `SixLabors.ImageSharp`, `System.Numerics.Tensors`.
  - Add project reference to `../../src/ElBruno.LocalEmbeddings/ElBruno.LocalEmbeddings.csproj` (for cosine similarity utilities).
  - Add the project to `ElBruno.LocalEmbeddings.slnx`.

- [ ] **Implement CLIP BPE tokenizer (`ClipTokenizer.cs`)**
  - Implement a minimal BPE tokenizer compatible with CLIP's vocabulary.
  - Load vocabulary and merge rules from `vocab.json` and `merges.txt` (standard CLIP tokenizer files from HuggingFace).
  - Handle special tokens: `<|startoftext|>` (SOT = 49406) and `<|endoftext|>` (EOT = 49407).
  - Pad/truncate to CLIP's context length (77 tokens).
  - Return `int[]` token IDs and `int[]` attention mask.

- [ ] **Implement CLIP text encoder (`ClipTextEncoder.cs`)**
  - Load the CLIP text encoder ONNX model (`text_model.onnx` or equivalent).
  - Accept a text string, tokenize it using `ClipTokenizer`, and run ONNX inference.
  - Extract the text embedding from the model output (typically the pooled output or the `[EOS]` token embedding).
  - L2-normalize the output embedding to enable cosine similarity via dot product.
  - Implement `IDisposable` for ONNX session cleanup.

- [ ] **Implement CLIP image encoder (`ClipImageEncoder.cs`)**
  - Load the CLIP vision encoder ONNX model (`vision_model.onnx` or equivalent).
  - Accept an image file path, load and preprocess the image:
    - Resize to 224×224 (CLIP's expected input size).
    - Center crop if needed.
    - Normalize pixel values using CLIP's mean `[0.48145466, 0.4578275, 0.40821073]` and std `[0.26862954, 0.26130258, 0.27577711]`.
    - Convert to `float[1, 3, 224, 224]` tensor (NCHW format).
  - Run ONNX inference and extract the image embedding.
  - L2-normalize the output embedding.
  - Implement `IDisposable` for ONNX session cleanup.

- [ ] **Implement search engine (`ImageSearchEngine.cs`)**
  - On startup, scan a directory for image files (`.jpg`, `.jpeg`, `.png`, `.bmp`, `.gif`).
  - Generate embeddings for all images using `ClipImageEncoder` (with progress indication).
  - Store image paths and their embeddings in memory.
  - Provide a `SearchAsync(string query, int topK)` method that:
    1. Generates text embedding using `ClipTextEncoder`.
    2. Computes cosine similarity between the text embedding and all image embeddings.
    3. Returns top-K results sorted by similarity score.
  - Use `EmbeddingExtensions.CosineSimilarity` from the main library for similarity computation where possible.

- [ ] **Implement main program (`Program.cs`)**
  - Parse command-line arguments for image folder path and model path.
  - Download or locate CLIP ONNX model files (document manual download steps).
  - Initialize `ClipTextEncoder`, `ClipImageEncoder`, and `ImageSearchEngine`.
  - Index all images in the specified folder.
  - Enter interactive query loop:
    1. Prompt user for a natural language query.
    2. Search and display top-5 matching images with similarity scores.
    3. Continue until user types "exit" or "quit".
  - Properly dispose all resources on exit.

- [ ] **Add sample images and documentation**
  - Create `samples/ImageSearchSample/images/README.md` with instructions for obtaining sample images.
  - Do NOT commit actual images to the repository (they are large). Provide download instructions or a script.
  - Create `samples/ImageSearchSample/README.md` with:
    - Overview of what the sample demonstrates.
    - Prerequisites (model download instructions using `optimum` CLI or direct HuggingFace download).
    - Step-by-step run instructions.
    - Expected output format.
    - Notes about CLIP model variants and dimension sizes.

- [ ] **Update solution and docs**
  - Add `ImageSearchSample` to `ElBruno.LocalEmbeddings.slnx`.
  - Update `samples/README.md` with the new sample entry.
  - Update roadmap item #12 status (mark as in-progress or completed).

- [ ] **Validation**
  - Verify the project builds: `dotnet build samples/ImageSearchSample/`.
  - Run the sample end-to-end with a small set of test images.
  - Verify search results are semantically meaningful (e.g., "a cat" returns cat images).

## CLIP Model Details

### Recommended model: `openai/clip-vit-base-patch32`

- **Text encoder dimensions:** 512
- **Image encoder dimensions:** 512
- **Context length:** 77 tokens
- **Image size:** 224×224 pixels
- **Vocabulary size:** 49408
- **Architecture:** ViT-B/32 (Vision Transformer with 32×32 patches)

### ONNX Export

Users can export the model to ONNX using the `optimum` CLI:

```bash
pip install optimum[exporters]
optimum-cli export onnx --model openai/clip-vit-base-patch32 ./clip-model/
```

This produces:
- `text_model.onnx` — Text encoder
- `vision_model.onnx` — Vision encoder (image encoder)
- `tokenizer.json`, `vocab.json`, `merges.txt` — Tokenizer files

Alternatively, pre-exported ONNX models are available from `Xenova/clip-vit-base-patch32` on HuggingFace.

## Key Design Decisions

1. **Separate from main library:** The CLIP text encoder uses BPE tokenization (not WordPiece), which is incompatible with the library's current `Tokenizer` class. The sample will implement its own `ClipTokenizer` rather than modifying the core library.

2. **No library changes required:** This sample demonstrates extensibility without modifying `ElBruno.LocalEmbeddings` core code. It reuses only the cosine similarity utilities from `EmbeddingExtensions`.

3. **Image processing via ImageSharp:** `SixLabors.ImageSharp` is chosen for cross-platform image loading (works on Windows, Linux, macOS without system dependencies).

4. **Manual model download:** Unlike the main library's automatic model download, CLIP models require separate text/vision ONNX files. The sample will document manual download steps rather than implementing automatic CLIP model downloading (which would be a significant undertaking).

## Risks and Mitigations

| Risk | Mitigation |
|------|------------|
| CLIP ONNX export produces incompatible model format | Test with multiple export methods; document exact versions |
| BPE tokenizer implementation complexity | Keep it minimal — only support ASCII + basic Unicode; reference HuggingFace tokenizer tests |
| Large model files in repository | Do NOT commit models; provide download instructions |
| Image preprocessing mismatch | Validate against Python CLIP reference implementation with identical test images |
| Performance on large image collections | Document expected indexing time; suggest batch processing for >1000 images |

## Future Enhancements (Out of Scope)

- Automatic CLIP model download integration in the library
- Tokenizer abstraction in the core library to support both WordPiece and BPE
- GPU acceleration for image encoding
- Persistent image index (save/load pre-computed image embeddings)
- Web UI for visual search results
